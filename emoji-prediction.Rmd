---
title: "Emoji prediction based on tweet text"
author: "Tijs Van den Heuvel"
date: "Published on `r Sys.Date()`"
output:
  html_document:
    df_print: paged
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(reticulate)
library(magrittr)
library(data.table)
```


# Tweet analysis and emoji prediction

This report describes the results of a research to predict emotion from tweets. 

More in particular the goal of this project was to predict an emoji based on text from a tweet.

When looking for similar projects, they get to an accuracy of 33%, this will be the success criterion.

> [emoji-prediction](https://github.com/vedantpuri/emoji-prediction) _Vedant Puri_, _Shubham Mehta_ (2018) 

It would be great if an algorithm could suggest some emojis based on a different piece of text.

The data set consists of a collection of tweets with at least one emoji.

[data source](https://www.kaggle.com/rexhaif/emojifydata-en)

The data is gathered, selected and reformatted by _Daniil Larionov_ from the _ArchiveTeam Twitter data_ source

I transformed this data into a data table with the use of PySpark and regex in this jupyter notebook: 

[preprocess](tweet-pre-process.ipynb)

- I spend a lot of time making this fast enough for large data sets
- I did some initial experimentation and data cleaning with a 10 tweets set
- Then I created R code to construct a model with a 165 tweets set
- When that model worked (with an accuracy of 18%) I proceeded to fix the pre-processor to convert the entire data set

## read the data 

### dev set 

This set was used for most of the development of this notebook.

This set contains 1.3 million tweets

```{r load-dev-dataset, eval=FALSE}
tweetDT <- fread("dev.gz")
```

### Full set

This is the full data set of 6.1 million tweets, it was used for training of the final model.

The data was already split into a training and a test set but this is unpractical for cleaning and one hot encoding of output,

so it is added together and splitted later.

```{r load-full-dataset, eval=TRUE}
tweetDT <- rbind( fread("./data/train.gz"), fread("./data/test.gz") )
```

## prepare the data

### a sample of the input data

```{r raw-input-sample}
smpl <- sample.int( nrow(tweetDT), 5 )
tweetDT[smpl]
```

### Clean the tweet text

There are a lot of weird characters in the tweet text.

They are removed for simplicity, since we want to focus on the actual words.

- convert all text to lowercase
- only keep letters a-z

```{r tweet-text-cleaning}
tweetDT[, text := text %>% tolower]
tweetDT$text <- gsub( '[^a-z]+', ' ', tweetDT$text )
```

### a sample of the cleaned tweets

```{r clean-text-sample}
tweetDT[smpl]
``` 

## data exploration

Here we examine how many words there are per tweet.

This is important to determine the size of the input layer, which will be a text vectorization.

The eventual decided length is 20 words per tweet since most tweets have fewer words and longer tweet size requires more computation.

```{r tweet-length-summary}
text_as_list_of_words <- tweetDT$text %>% strsplit(" ") 
text_as_list_of_words %>% sapply(length) %>% summary
```

```{r tweet-length-plot}
text_as_list_of_words %>% sapply(length) %>% hist( breaks=55, main="Distribution of words per tweet" ) 
```

### total vocabulary size

Here we examine the total amount of different words in all the tweets.

We could use this to determine the size of the vocabulary used by the text vectorization layer but we don't.

This is just an exploratory step.

It is useful for observing the effect of stemming which reduces the total amount of different words from 613,993 words to 529,416

```{r word-count}
text_as_list_of_words %>% unlist %>% unique %>% length
```

### emoji count

Here we count the amount of different emoji's.

this will determine the size of the output layer of the network.

```{r emoji-count}
emoji_count <- unique( tweetDT$emoji ) %>% length
emoji_count
```

### how many tweets per emoji

In this last exploratory step we look at the distribution of tweets per emoji.

In the most balanced situation we would have 6170786/49 = 125,934 tweets per emoji

The most popular emoji `emoji_freq[emoji_freq$Freq%>%order%>%rev%>%head(1),]` has `max(emoji_freq$Freq)` tweets.

The least popular emoji `emoji_freq[emoji_freq$Freq%>%order%>%head(1),]` has only `min(emoji_freq$Freq)` tweets.

The Median is lower than the Mean, this means that most tweets have fewer emoji's than the average.

```{r average-amount-of-tweets-per-emoji}
emoji_freq<-as.data.frame(table(tweetDT$emoji))

emoji_freq$Freq%>%summary
```

Here we see which tweets exactly are to most and least common.

This can be referenced when checking the validity of made predictions.

If the model predicts 'face_with_tears_of_joy' every time it would score an accuracy of 16.37%.

```{r amount-of-tweets-per-emoji}
bottom <- emoji_freq$Freq%>%order%>%head(10)

top <- emoji_freq$Freq%>%order%>%rev%>%head(10)

emoji_freq[top,]

emoji_freq[bottom,]
```
Here we see that some emoji's have a lot more tweets, but most emoji's have a proportionate amount

```{r tweets-per-emoji-distribution}
tweetDT$emoji %>% table %>% sort %>% rev %>% barplot
```

## Text classification with TensorFlow

Let's build a model to predict which tweet goes with which emoji.

I started from this TensorFlow [tutorial](https://tensorflow.rstudio.com/tutorials/beginners/basic-ml/tutorial_basic_text_classification/)

The input is the text of the tweet.

The output is the emoji from that tweet.

A neural network takes in tensors and not text so the data needed to be prepared.

```{r tensorflow-setup, message=FALSE}
library(keras)
library(dplyr)
library(ggplot2)
library(purrr)
```

### prepare the input data

To encode the text data into a tensor we use the TensorFlow text vectorization layer.

It creates a dictionary to represent each of the 10,000 most common words by an integer.

The algorithm works as follows:

- Split tweet into array of words.
- Pad the arrays so they all have same length.
- Recombine words into tokens to create integer tensor of shape `num_words * max_length`.
- Adapt the text vectorization layer to learn the vocabulary of the data set.

We need to use an embedding layer capable of handling this shape as first layer,

since this is a built-in TensorFlow function we can use the standard keras embedding layer.

the size of 10,000 was experimentally chosen to be the most effective amount for accuracy/ computation.

Because this is a rather intensive computation, 

we could try to load the text vectorization object from file but RDS doesn't work because it is a Tensorflow Tensor and

Tensorflow sessions in R are a bit finicky.


```{r adapt-text-vectorization-layer ,include=TRUE}
num_words <- 10000
max_length <- 20
text_vectorization <- layer_text_vectorization(
  max_tokens = num_words, 
  output_sequence_length = max_length, 
)
text_vectorization %>% adapt(tweetDT$text)
 
```

#### compare max_length variations

- amount of words: 10000
- model size: 192(input),192(dense),192(gru)
- 20 epochs

|max_length | minutes/epoch | accuracy|
|---|---|---|
|32|9.8|27.89|
|25|7.6|27.91|
|20|6.3|27.85|

#### text vectorization insight

Here is shown how the text vectorization layer transforms the input

```{r text-vectorization-input-transformation}
tweetDT$text[7]

text_vectorization(matrix(tweetDT$text[7], ncol = 1))
```

### prepare the output data

The emojis also have to be encoded as a tensor.

One hot encoding (or dummy variables) is a decent method size there are only 49 different emoji's.

The Keras library has a built-in function: to_categorical but it needs an integer input.

Thus we convert the emoji names to integers.

The first column is entirely empty so we remove it.

Create an emoji dictionary to find which number maps to which emoji after prediction.

```{r vectorize-emoji-as-one-hot}
emoji_int <- match(tweetDT$emoji, unique(tweetDT$emoji))

one_hot_emoji <- to_categorical(emoji_int)
one_hot_emoji <- one_hot_emoji[, 2:(emoji_count+1)]

emoji_dict <- tweetDT$emoji %>% unique
```

### split the dataset into training & testing

Here we split the data in to a training and testing set.

The division is 80% training and 20% testing, and is arbitrarily chosen.

```{r split-data-in-test-and-training-set}
training_id <- sample.int( nrow(tweetDT), size = nrow(tweetDT)*0.8 )
training <- tweetDT[ training_id, ]
testing <- tweetDT[ -training_id, ]
oh_train <-one_hot_emoji[ training_id, ]
oh_test <- one_hot_emoji[ -training_id, ]

nrow(training)
nrow(testing)
```

### design the model

In the design of the model we have to answer a couple of questions.

- how many layers?
- how many hidden units?
- what types of building blocks?
- what type of architecture?

#### model 1 FFA

I started with the model from the tutorial, added an extra dense layer of size 16, tried bigger layers of size 32 and 50, tried more layers, up to 5.

Then I removed most of the dropout because that made the accuracy increase, I didn't realize that this was just overfitting

Then I added more dropout after each layer and had an accuracy of 38%.

Then I realized there were duplicate rows because some tweets had multiples of the same emoji.

This caused the problem that the same record was in the training and test set

When removing the duplicates, the accuracy dropped to 23%.

```{r base-FFA, eval = FALSE}
input <- layer_input( shape = c( 1 ), dtype = "string" )

output <- input %>% 
  text_vectorization %>% 
  layer_embedding( input_dim = num_words + 1, output_dim = 16 ) %>%
  layer_global_average_pooling_1d %>%
  layer_dense( units = 16, activation = "relu" ) %>%
  layer_dropout( 0.5 ) %>% 
  layer_dense( units = emoji_count, activation = "softmax" )

model <- keras_model( input, output )
```

How many layers ?

tested with 50 unit layers

| # of dense layers | accuracy |
|---|---|
| 1 | 24.6% |
| 2 | 24.0% |
| 3 | 23% |
| 4 | 22% |

how many units per layer? Tested with an embedding layer and 1 dense layer.

|embedding units | dense units | accuracy |
|---|---|---|
| 40 | 50 | 24.36% |
| 40 | 45 | 24.5% |
| 50 | 50 | 24.6% |
| 55 | 50 | 24.45% |
| 55 | 55 | 24.58% |
| 60 | 55 | 24.60% |
| 70 | 60 | 24.68% |
| 80 | 70 | 24.56% |

```{r best-FFA, eval = FALSE}
input <- layer_input( shape = c( 1 ), dtype = "string" )

output <- input %>% 
  text_vectorization %>% 
  layer_embedding( input_dim = num_words + 1, output_dim = 70 ) %>%
  layer_global_average_pooling_1d %>%
  layer_dense( units = 60, activation = "relu" ) %>%
  layer_dropout( 0.4 ) %>% 
  layer_dense( units = emoji_count, activation = "softmax" )

model <- keras_model( input, output )
```

#### model 2 LSTM

Because tweets consist of a sequence of words we can apply recurrent neural network methods.

The most popular RNN architecture are LSTM's, they are ideal from large sequences.

For this project they are a bit overkill since the sequences are quite short: 20 words / tweet.

GRU's outperform LSTM's on time and accuracy for small sequences, so this will be the next step.

how many units per layer?

| embedding units | LSTM units | accuracy |
|---|---|---|
| 50 | 50 | 24.99% |
| 60 | 60 | 25.15% |
| 70 | 70 | 24.97% |

how many layers ? Tested with 60 units layers

|number of layers| accuracy |
|---|---|
| 1 | 25.15% |
| 2 | 24.58% |

```{r LSTM, eval = FALSE, warning = FALSE}
input <- layer_input( shape = c( 1 ), dtype = "string" )

output <- input %>% 
  text_vectorization %>% 
  layer_embedding( input_dim = num_words + 1, output_dim = 60 ) %>%
  layer_lstm( units = 60, dropout = 0.5, recurrent_dropout = 0.5 )%>%
  layer_dense( units = emoji_count, activation = "softmax" )

model <- keras_model( input, output )
```

#### model 3 GRU

GRU's are similar too but less complex than LSTM's which makes them twice as fast. 

| amount of layers | embedding units | GRU units | accuracy |
|---|---|---|---|
| 1 | 60 | 60 | 24.88% |
| 1 | 70 | 60 | 25.01% |
| 1 | 80 | 70 | 25% |
| 1 | 90 | 70 | 24.97% |

The amount of nodes doesn't seem to change all that much.

Dropout and recurrent dropout inside of the GRU layer makes it incompatible with the CUDNN Nvidia neural network accelerator library, which makes the computation a lot slower.

With recurrent dropout one epoch takes 9 minutes with an accuracy of 26.15% after 10 epochs.

Without recurrent dropout one epoch takes 6.5 minutes with an accuracy of 26.78% after 11 epochs.

```{r first-GRU, eval = FALSE}
input <- layer_input( shape = c( 1 ), dtype = "string" )

output <- input %>% 
  text_vectorization %>% 
  layer_embedding( input_dim = num_words + 1, output_dim = 64 ) %>%
  layer_gru( units = 64, dropout = 0.5, recurrent_dropout = 0.5 )%>%
  layer_dense( units = emoji_count, activation = "softmax" )

model <- keras_model( input, output )
```

```{r best-one-layer-GRU, eval = FALSE}
input <- layer_input( shape = c( 1 ), dtype = "string" )

output <- input %>% 
  text_vectorization %>% 
  layer_embedding( input_dim = num_words + 1, output_dim = 64 ) %>%
  layer_gru( units = 64 )%>%
  layer_dropout( 0.5 ) %>% 
  layer_dense( units = emoji_count, activation = "softmax" )

model <- keras_model( input, output )
```

| number of words | embedding units | GRU units | minutes / epoch | #epochs to over fit | accuracy|
|---|---|---|---|---|---|
| 20000 | 64 | 64 | 8.5 | 7 | 26.75% |
| 20000 | 128 | 128 | 8.5 | 4 | 27.29% |
| 10000 | 128 | 128 | 7.8 | 6 | 27.25% |
| 10000 | 128 | 128, 64 | 13.7 | 7 | 27.37% |
| 10000 | 128 | 192 | 8.6 | 5 | 27.46 % |
| 10000 | 192 | 192 | 9.3 | 5 | 27.64 % |
| 10000 | 256 | 192 | 10 | 4 | 27.53 % |
| 10000 | 256 | 256 | 11 | 4 | 27.63 % |
| 10000 | 192 | 256 | 10.5 | 4 | 27.54 % |
| 10000 | 192 | 192, 128(dense)| 9.3 | 7 | 27.26 % |
| 10000 | 192 | 192(dense), 192| 10.5 | +15 | 27.82% |

```{r best-GRU-model, warning = FALSE}
input <- layer_input( shape = c( 1 ), dtype = "string" )

output <- input %>% 
  text_vectorization %>% 
  layer_embedding( input_dim = num_words + 1, output_dim = 192 ) %>%
  layer_dense( units = 192 ) %>%
  layer_dropout( 0.4 ) %>%
  layer_gru( units = 192 ) %>%
  layer_dropout( 0.5 ) %>% 
  layer_dense( units = emoji_count, activation = "softmax" )

model <- keras_model( input, output )
```
 
#### model 4 bidirectional GRU

Bidirectional GRU's connect two hidden layers of opposite directions to the same output.

Which means that the output layer can get information from past (backwards) and future (forward) states simultaneously.

This is just something I found in the literature and wanted to try.

The results have no improvement and are much slower.

- 14.3 minutes / epoch
- over fits in 5 epochs
- accuracy 27.27%

```{r bidirctional-GRU, eval=FALSE}
input <- layer_input( shape = c( 1 ), dtype = "string" )

output <- input %>% 
  text_vectorization %>% 
  layer_embedding( input_dim = num_words + 1, output_dim = 128 ) %>%
  bidirectional( layer_gru( units = 128 )) %>%
  layer_dropout( 0.5 ) %>% 
  layer_dense( units = emoji_count, activation = "softmax" )

model <- keras_model( input, output )
```

### compile the model 

Before training, a model needs to be compiled with an optimizer and loss function.

The tutorial uses binary crossentropy but our variables are categorical, therefore we use categorical crossentropy.

```{r compile-the-model}
model %>% compile(
  optimizer = 'adam',
  loss = 'categorical_crossentropy',
  metrics = list( 'accuracy' )
)
```

### Train the model

Here we train the model. 

The input is a tweet text from the training set.

The output is a one hot representation of which emoji belongs to the input tweet.

The amount of epochs is 20 because that is how long the model can train without the loss becoming higher than the validation loss.

The batch size is 1660 because that is how many CUDA cores my GPU has. 

When training on computers without dedicated NVIDIA GPU's, a batch size of 512 is recommended.

Try to load model from file, when that fails or the model doesn't exist, train a new model.

```{r training, include=FALSE}

train <- function() {
  history <- model %>% fit(
  training$text,
  oh_train,
  epochs = 10,
  batch_size = 1660, 
  validation_split = 0.2,
  verbose=2
  ) 
  model %>% save_model_tf( "my_model" )
  saveRDS(history,'learn-history.rds')
}

if(file.exists("my_model")){
  tryCatch( model <- load_model_tf( "my_model" ) , error = function( e ){ train() } )
  
}else { train() }
```

```{r plot-training-history}
if( exists( 'learn-history.rds' ) ){
  history <- readRDS( 'learn-history.rds' )
  plot( history )
}
```

### evaluate model

Here we evaluate the model by calculating the accuracy on the test set.

```{r evaluate-model}
results <- model %>% evaluate( testing$text, oh_test, verbose = 0 )
results
```

## results

### single prediction

This is a direct application of the model and can also be applied to any piece of text.

```{r single-prediction }

id <- 9
prediction1 <- predict( model, testing$text[ id ] )[1,]

any_text <- 'i like all things'
prediction2 <- predict( model, any_text,)

print_predictions <- function( prediction,text,from_test_set){
  
  amount <-10
  top_val <- prediction[ order( prediction ) ] %>% rev %>% head( amount ) %>% round(3)
  top_ids <- order( prediction ) %>% rev %>% head( amount )

  top_emoji <- emoji_dict[ top_ids ]
  top_prediction <- cbind( top_emoji, top_val )

  
  print( paste( "text:", text ))
  
  if(from_test_set){
    print( paste( "true:", testing$emoji[ id ] ))
  }
  
  print( top_prediction )
}

print_predictions( prediction1,testing$text[ id ],TRUE)
print_predictions( prediction2,any_text,FALSE)
```

## variations

### check effect of stemming

Let's try stemming as a technique to reduce the total amount of different words.

This would mean that the information in the 10,000 words of the text vectorization layer is more dense.

Let's see if this has an impact on model performance

1. stem
2. text vectorization
3. split data 
4. train model
5. compare to score without stemming

For this test we take the best performing GRU model.

Without stemming, the model takes 10.5 minutes per epoch to reach an accuracy of 27.82% after 15 epochs.

With stemming, the model takes 9.8 minutes per epoch to reach an accuracy  of 27.82% after 15 epochs.

There is no improvement in accuracy but it computes a little faster.

```{r apply-stemmer, include=FALSE}
library(tm)
stemmedtext <- stemDocument( tweetDT$text )
```

```{r sample-stemmed-text, include=FALSE}
stemmedtext[ sample(10) ]
```

## memory cleanup

At this stage I ran in to a lot of memory issues, the objects where just too large.

We will remove some object we don't need anymore and 

continue with a subset of the data to be able to continue this notebook.

```{r memory-cleanup,include=FALSE}
subset_tenth <- sample.int( nrow(tweetDT), nrow(tweetDT)/10 )
subset_1k <- sample.int( nrow(tweetDT), 1000 )

tweetDT_subset <- tweetDT[subset_tenth]
sampleDT <-  tweetDT[subset_1k]

rm(tweetDT)
rm( text_as_list_of_words)
gc()
```

## n-grams

A different route is to look at n-grams, an n-gram is a contiguous sequence of n items from a given sample of text.
In this case we use character 3-grams which means that we split the text of an emoji in sequences of three characters.

We use the quanteda library for the construction of the n-grams

```{r quanteda, include = TRUE}
library(quanteda)
```

### tweet text as list of ngrams instead of list of words

Here we keep a sentence structure but with 3-grams instead of words, to be used with the text_vectorization layer. 

```{r ngrams-as-a-string}
clean <- function( tweet ){
  return( paste( tweet, collapse=' ') )
}

gram <- tokens( tweetDT_subset$text, "character") %>% tokens_ngrams( n=3, concatenator='') %>% lapply( clean )

tweet_ngram_DT <- cbind( tweetDT_subset, as.character( gram ))[,c(1,3)]

rm(gram)
```

Analog to the words we examine how many 3-grams there are per tweet, to determine the max length of the text vectorization layer. 

```{r ngrams-per-tweet}
text_as_list_of_3grams <- tweet_ngram_DT$V2 %>% strsplit(" ") 
text_as_list_of_3grams%>% sapply(length) %>% summary
```

Here we see that the optimal length of a tweet as a sentence of 3-grams is around 100.

```{r ngrams-per-tweet-plot}
text_as_list_of_3grams %>% sapply(length) %>% hist(breaks=162,main="Distribution of ngrams per tweet")
```

Here we check the amount of n-grams in the data set.

There are 16625 different 3-grams in the dev set.

There are 229941 different 4-grams in the dev set.

There are around 16 thousand different 3-grams in the sub set.

Even though the subset contains a mere tenth of the full data set, 

this amount of different 3-grams will not be much more 

since the total amount of possible 3-grams is 17576 (26^3).

```{r amount-of-ngrams, eval = FALSE}
text_as_list_of_3grams %>% unlist %>% unique %>% length 

rm( text_as_list_of_3grams)
```

This would be the appropriate text vectorization layer to be used with 3-grams.

```{r text-vectorization-layer-3-grams ,include=FALSE,eval=FALSE}
num_words <- 10000
max_length <- 100
text_vectorization <- layer_text_vectorization(
  max_tokens = num_words, 
  output_sequence_length = max_length
  )
text_vectorization %>% adapt( tweetDT$V2 )
```

#### model results

GPU memory cant handle bigger model nor the full data set so the dev set is used here.

| n-gram | input size | dense layer size | gru layer size | seconds / epoch | # epochs | accuracy|
|---|---|---|---|---|---|---|
| 3 | 100 | 192 | 192 | / | 5 | 24.35% |
| 3 | 100 | 222 | 222 | 380 | 6 | 24.59% |
| 3 | 100 | 128 | 128,128 | 560 | 10 | 24.57% |
| 4 | 100 | 192 | 192 | 360 | 6 | 24.64 |

### 3-grams as one hot matrix

Another option would be to encode the 3-grams as one hot (or dummy variables).

Here we demonstrate a proof of concept but the entire data set would generate a sparse 0-1 matrix of `nrow(tweetDT)` x 16,625 dimensions, which my computer couldn't handle at all.

Even a one hot matrix of 10,000 tweets is 379 MB with a sparse representation of 5.8 MB

A one hot matrix of 100,000 tweets is already 5.5 GB dense and my computer could not create a sparse representation due to RAM limitations.

Since this is merely an illustration, a sample set of 1000 tweets will be used.

```{r ngrams-as-one-hot}
ngrams <- tokens(sampleDT$text,"character")%>% tokens_ngrams(n=3,concatenator='')

uniquengrams <- ngrams %>% unlist %>% unique

ngrams<-lapply(ngrams,match,table=uniquengrams)

oh<- do.call(rbind, lapply(ngrams, function(i) as.integer(!is.na(match(unique(unlist(ngrams)), i)))))
```

```{r sparse-representation}
library(Matrix)

oh_sparse <-Matrix( data=oh, sparse=TRUE )
```

#### machine learning model for 3-gram oh matrix

```{r ngram-data-prepare,eval=TRUE}
emoji_int <- match(sampleDT$emoji, unique(sampleDT$emoji))

one_hot_emoji_ngram <- to_categorical(emoji_int)
one_hot_emoji_ngram <- one_hot_emoji_ngram[, 2:(emoji_count+1)]

one_hot_emoji_ngram <- to_categorical(emoji_int)[, 2:(emoji_count+1)]

training_id <- sample.int( nrow(sampleDT), size = nrow(sampleDT)*0.8 )

oh_training <- oh[ training_id, ]
oh_testing <- oh[ -training_id, ]

emoji_training_nrgam <-one_hot_emoji_ngram[ training_id, ]
emoji_testing_ngram <- one_hot_emoji_ngram[ -training_id, ]
```

```{r ngram-model,eval=TRUE,include=FALSE}
input <- layer_input( shape = c( ncol(oh) ), dtype = "int32" )

output <- input %>% 
  layer_dense( 1024 ) %>%
  layer_dropout( 0.5 ) %>%
  layer_dense( 512 ) %>%
  layer_dropout( 0.5 ) %>%
  layer_dense( units = emoji_count, activation = "softmax" )

model <- keras_model( input, output )
```

```{r compile-ngram-model,eval=TRUE,include=FALSE}
model %>% compile(
  optimizer = 'adam',
  loss = 'categorical_crossentropy',
  metrics = list( 'accuracy' )
)
```

```{r training-ngram, eval=TRUE,include=FALSE}
history <- model %>% fit(
  oh_training,
  emoji_training_nrgam,
  epochs = 5,
  batch_size = 830, 
  validation_split = 0.1,
  verbose=2
) 
```

```{r evaluate-ngram-model,eval=TRUE}
results <- model %>% evaluate(oh_testing, emoji_testing_ngram, verbose = 0 )
results
```

## conclusion

The goal of this project was to predict an emoji based on text from a tweet.

The initial goal was to reach an accuracy of 33%, the highest accuracy we achieved was 27.82%, this means that the official criterion has not been met. 

We have to take into account that there are 49 different outcomes so the random guess would achieve a mere 2%.

The most effective way of encoding the text data was the Tensorflow Text Vectorization layer.

We also looked at character 3-grams to be one hot encoded, 
since this is an accurate binary representation of text data which neural networks prefer.

Another advantage of character n-grams is that the total vocabulary is reduced from 613,993 different words to 16,625 different 3-grams. 

The problem with one hot encoding of these 3-grams is that we get a matrix of 6.1 million x 16,625 dimensions which is to large for my computer memory, even in a sparse representation. 

This obstacle could be overcome in future projects by partially loading and constructing the matrix.

We found that GRU networks are the best architecture for this particular problem. We get the advantage of reccurenct neural networks without the complexity of LSTM's. 



















