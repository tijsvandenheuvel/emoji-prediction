tweetDT[sample(1:.N,10)]
tweetDT[sample(1:.N,10)]
tweetDT$text %>% strsplit(" ") %>% sapply(length) %>% summary()
tweets_reduced_single_emoji <- fread("./data/reduced_tweets_single.csv")
tweets_165_single_emoji <- fread("./data/165tweets.csv")
tweetDT <- tweets_165_single_emoji
nrow(tweets_165_single_emoji)
tweetDT$text<-gsub('[^\x01-\x7F]',' ',tweetDT$text)
tweetDT$text<-gsub('[0-9]+',' ',tweetDT$text)
#tweetDT$text<-gsub('\\b\\w{1,2}\\s','',tweetDT$text)
tweetDT$text<-gsub('\\s+',' ',tweetDT$text)
tweetDT$text<-gsub(' $','',tweetDT$text)
tweetDT$text<-tolower(tweetDT$text)
tweetDT$text %>% strsplit(" ") %>% sapply(length) %>% summary()
training_id <- sample.int(nrow(tweetDT), size = nrow(tweetDT)*0.8)
training <- tweetDT[training_id,]
testing <- tweetDT[-training_id,]
nrow(training)
nrow(testing)
num_words <- 1000
max_length <- 50
text_vectorization <- layer_text_vectorization(
max_tokens = num_words,
output_sequence_length = max_length,
)
text_vectorization %>% adapt(tweetDT$text)
get_vocabulary(text_vectorization)
tweetDT$text<-gsub('[^\x01-\x7F]',' ',tweetDT$text)
tweetDT$text<-gsub('[0-9]+',' ',tweetDT$text)
tweetDT$text<-gsub('\\b\\w{1,2}\\s','',tweetDT$text)
tweetDT$text<-gsub('\\s+',' ',tweetDT$text)
tweetDT$text<-gsub(' $','',tweetDT$text)
tweetDT$text<-tolower(tweetDT$text)
tweetDT$text %>% strsplit(" ") %>% sapply(length) %>% summary()
training_id <- sample.int(nrow(tweetDT), size = nrow(tweetDT)*0.8)
training <- tweetDT[training_id,]
testing <- tweetDT[-training_id,]
nrow(training)
nrow(testing)
num_words <- 1000
max_length <- 50
text_vectorization <- layer_text_vectorization(
max_tokens = num_words,
output_sequence_length = max_length,
)
text_vectorization %>% adapt(tweetDT$text)
get_vocabulary(text_vectorization)
?fit
?fit.keras.engine.training.Model
model %>% compile(
optimizer = 'adam',
loss = 'binary_crossentropy',
metrics = list('accuracy')
)
input <- layer_input(shape = c(1), dtype = "string")
output <- input %>%
text_vectorization() %>%
layer_embedding(input_dim = num_words + 1, output_dim = 16) %>%
layer_global_average_pooling_1d() %>%
layer_dense(units = 16, activation = "relu") %>%
layer_dropout(0.5) %>%
layer_dense(units = 1, activation = "sigmoid")
model <- keras_model(input, output)
model %>% compile(
optimizer = 'adam',
loss = 'binary_crossentropy',
metrics = list('accuracy')
)
history <- model %>% fit(
training$text,
training$emoji,
epochs = 10,
batch_size = 13,
validation_split = 0.2,
verbose=2
)
model %>% compile(
optimizer = 'adam',
loss = 'crossentropy',
metrics = list('accuracy')
)
history <- model %>% fit(
training$text,
training$emoji,
epochs = 10,
batch_size = 13,
validation_split = 0.2,
verbose=2
)
oh_train <- model.matrix(~0+tweetDT[train,'emoji'])
oh_train <- model.matrix(~0+tweetDT[training,'emoji'])
oh_train <- model.matrix(~0+tweetDT['emoji'])
oh_train <- model.matrix(~0+tweetDT[training_id,'emoji'])
oh <- one_hot(tweetDT)
install.packages("mltools")
library(mltools)
oh <- one_hot(tweetDT)
library(mltools)
oh <- one_hot(tweetDT)
oh
library(mltools)
oh <- one_hot(tweetDT$emoji)
oh
?one_hot
library(mltools)
oh <- one_hot(tweetDT,col="emoji")
library(mltools)
oh <- one_hot(tweetDT,cols="emoji")
tweetDT[sample(1:.N,10)]
library(mltools)
oh <- one_hot(tweetDT,cols=3)
library(mltools)
oh <- one_hot(tweetDT,cols='emoji')
library(mltools)
#oh <- one_hot(tweetDT,cols='emoji')
oh <- model.matrix(~0+emoji)
library(mltools)
#oh <- one_hot(tweetDT,cols='emoji')
oh <- model.matrix(~emoji,tweetDT)
oh
history <- model %>% fit(
training$text,
oh,
epochs = 10,
batch_size = 13,
validation_split = 0.2,
verbose=2
)
oh_train <- model.matrix(~emoji,training)
oh_test <- model.matrix(~emoji,testing)
levels(oh_train)
oh_train <- model.matrix(~emoji,training)
oh_test <- model.matrix(~emoji,testing)
levels(oh_train$emoji)
oh_train <- model.matrix(~emoji,training)
oh_test <- model.matrix(~emoji,testing)
levels(training$emoji)
oh_train <- model.matrix(~emoji,training)
oh_test <- model.matrix(~emoji,testing)
oh_train
oh_train <- model.matrix(~emoji,training)
oh_test <- model.matrix(~emoji,testing)
oh_train.describe()
oh_train <- model.matrix(~emoji,training)
oh_test <- model.matrix(~emoji,testing)
describe(oh_train)
oh_train <- model.matrix(~emoji,training)
oh_test <- model.matrix(~emoji,testing)
str(oh_train)
history <- model %>% fit(
training$text,
oh_train,
epochs = 10,
batch_size = 13,
validation_split = 0.2,
verbose=2
)
oh_train <- model.matrix(~emoji,training)
oh_test <- model.matrix(~emoji,testing)
str(oh_train)
levels(oh_train)
oh_train <- model.matrix(~emoji,training)
oh_test <- model.matrix(~emoji,testing)
str(oh_train)
ncol(oh_train)
oh_train <- model.matrix(~emoji,training)
oh_test <- model.matrix(~emoji,testing)
str(oh_train)
ncol(oh_train)
ncol(oh_test)
oh_train <- model.matrix(~emoji,training)
oh_test <- model.matrix(~emoji,testing)
oh <- model.matrix(~emoji,tweetDT)
ncol(oh_train)
ncol(oh_test)
ncol(oh)
library(mltools)
one_hot(tweetDT)
oh_train <- model.matrix(~emoji,training)
oh_test <- model.matrix(~emoji,testing)
oh <- model.matrix(~emoji,tweetDT)
ncol(oh_train)
ncol(oh_test)
ncol(oh)
library(mltools)
cbind(tweetDT,one_hot(tweetDT))
tweetDT[sample(1:.N,10)]
library(mltools)
oh <-one_hot(tweetDT)
cbind(tweetDT,oh)
tweetDT[sample(1:.N,10)]
?cbind
library(mltools)
oh <-one_hot(tweetDT)
tweetDT_oh<-cbind(tweetDT,oh)
tweetDT_oh[sample(1:.N,10)]
library(mltools)
oh <-one_hot(tweetDT)
oh
tweetDT_oh<-cbind(tweetDT,oh)
?one-hot
?one_hot
history <- model %>% fit(
training$text,
as.numeric(training$emoji == ":red_heart:"),
epochs = 10,
batch_size = 13,
validation_split = 0.2,
verbose=2
)
training_id <- sample.int(nrow(tweetDT), size = nrow(tweetDT)*0.8)
training <- tweetDT[training_id,]
testing <- tweetDT[-training_id,]
nrow(training)
nrow(testing)
num_words <- 1000
max_length <- 50
text_vectorization <- layer_text_vectorization(
max_tokens = num_words,
output_sequence_length = max_length,
)
text_vectorization %>% adapt(tweetDT$text)
num_words <- 1000
max_length <- 50
text_vectorization <- layer_text_vectorization(
max_tokens = num_words,
output_sequence_length = max_length,
)
text_vectorization %>% adapt(tweetDT$text)
get_vocabulary(text_vectorization)
input <- layer_input(shape = c(1), dtype = "string")
output <- input %>%
text_vectorization() %>%
layer_embedding(input_dim = num_words + 1, output_dim = 16) %>%
layer_global_average_pooling_1d() %>%
layer_dense(units = 16, activation = "relu") %>%
layer_dropout(0.5) %>%
layer_dense(units = 1, activation = "sigmoid")
model <- keras_model(input, output)
model %>% compile(
optimizer = 'adam',
loss = 'binary_crossentropy',
metrics = list('accuracy')
)
history <- model %>% fit(
training$text,
as.numeric(training$emoji == ":red_heart:"),
epochs = 10,
batch_size = 13,
validation_split = 0.2,
verbose=2
)
results <- model %>% evaluate(testing$text, as.numeric(testing$tag == "pos"), verbose = 0)
results <- model %>% evaluate(testing$text, as.numeric(testing$emoji == ":red_heart:"), verbose = 0)
results
plit(history)
plot(history)
results <- model %>% evaluate(testing$text, as.numeric(testing$emoji == ":red_heart:"), verbose = 0)
results
plot(history)
history <- model %>% fit(
training$text,
as.numeric(training$emoji == ":OK_hand:"),
epochs = 10,
batch_size = 13,
validation_split = 0.2,
verbose=2
)
results <- model %>% evaluate(testing$text, as.numeric(testing$emoji == ":OK_hand:"), verbose = 0)
results
plot(history)
?loss_categorical_crossentropy
model %>% compile(
optimizer = 'adam',
loss = 'loss_catagorical_crossentropy',
metrics = list('accuracy')
)
history <- model %>% fit(
training$text,
training$emoji,
epochs = 10,
batch_size = 13,
validation_split = 0.2,
verbose=2
)
library(mltools)
emoji_vectorization%>%adapt(tweetDT$emoji)
num_words <- 1000
max_length <- 50
emoji_vectorization <- layer_text_vectorization(
max_tokens = num_words,
output_sequence_length = max_length,
)
emoji_vectorization(matrix(tweetDT$emoji[1], ncol = 1))
num_words <- 1000
max_length <- 50
emoji_vectorization <- layer_text_vectorization(
max_tokens = num_words,
output_sequence_length = max_length,
)
library(mltools)
emoji_vectorization%>%adapt(tweetDT$emoji)
get_vocabulary(emoji_vectorization)
emoji_vectorization(matrix(df$emoji[1], ncol = 1))
emoji_vectorization(matrix(tweetDT$emoji[1], ncol = 1))
input <- layer_input(shape = c(1), dtype = "string")
output <- input %>%
text_vectorization() %>%
layer_embedding(input_dim = num_words + 1, output_dim = 16) %>%
layer_global_average_pooling_1d() %>%
layer_dense(units = 16, activation = "relu") %>%
layer_dropout(0.5) %>%
layer_dense(units = 50, activation = "sigmoid")
model <- keras_model(input, output)
model %>% compile(
optimizer = 'adam',
loss = 'loss_catagorical_crossentropy',
metrics = list('accuracy')
)
history <- model %>% fit(
training$text,
training$emoji,
epochs = 10,
batch_size = 13,
validation_split = 0.2,
verbose=2
)
input <- layer_input(shape = c(1), dtype = "string")
output <- input %>%
text_vectorization() %>%
layer_embedding(input_dim = num_words + 1, output_dim = 16) %>%
layer_global_average_pooling_1d() %>%
layer_dense(units = 16, activation = "relu") %>%
layer_dropout(0.5) %>% emoji_vectorization()
input <- layer_input(shape = c(1), dtype = "string")
output <- input %>%
text_vectorization() %>%
layer_embedding(input_dim = num_words + 1, output_dim = 16) %>%
layer_global_average_pooling_1d() %>%
layer_dense(units = 16, activation = "relu") %>%
layer_dropout(0.5) %>% emoji_vectorization()
num_words <- 1000
max_length <- 50
text_vectorization <- layer_text_vectorization(
max_tokens = num_words,
output_sequence_length = max_length,
)
text_vectorization %>% adapt(tweetDT$text)
get_vocabulary(text_vectorization)
model %>% compile(
optimizer = 'adam',
loss = 'catagorical_crossentropy',
metrics = list('accuracy')
)
input <- layer_input(shape = c(1), dtype = "string")
output <- input %>%
text_vectorization() %>%
layer_embedding(input_dim = num_words + 1, output_dim = 16) %>%
layer_global_average_pooling_1d() %>%
layer_dense(units = 16, activation = "relu") %>%
layer_dropout(0.5) %>% emoji_vectorization()
emoji_vectorization(matrix(tweetDT$emoji[1], ncol = 1))
input <- layer_input(shape = c(1), dtype = "string")
output <- input %>%
text_vectorization() %>%
layer_embedding(input_dim = num_words + 1, output_dim = 16) %>%
layer_global_average_pooling_1d() %>%
layer_dense(units = 16, activation = "relu") %>%
layer_dropout(0.5) %>% emoji_vectorization()%>%
layer_dense(units = 50, activation = "sigmoid")
input <- layer_input(shape = c(1), dtype = "string")
input
output <- input %>%
text_vectorization() %>%
layer_embedding(input_dim = num_words + 1, output_dim = 16) %>%
layer_global_average_pooling_1d() %>%
layer_dense(units = 16, activation = "relu") %>%
layer_dropout(0.5) %>% emoji_vectorization()%>%
layer_dense(units = 50, activation = "sigmoid")
input <- layer_input(shape = c(1), dtype = "string")
output <- input %>%
text_vectorization() %>%
layer_embedding(input_dim = num_words + 1, output_dim = 16) %>%
layer_global_average_pooling_1d() %>%
layer_dense(units = 16, activation = "relu") %>%
layer_dropout(0.5) %>%
#emoji_vectorization() %>%
layer_dense(units = 50, activation = "softmax")
model <- keras_model(input, output)
model %>% compile(
optimizer = 'adam',
loss = 'catagorical_crossentropy',
metrics = list('accuracy')
)
history <- model %>% fit(
training$text,
training$emoji,
epochs = 10,
batch_size = 13,
validation_split = 0.2,
verbose=2
)
history <- model %>% fit(
training$text,
emoji_vectorization(training$emoji),
epochs = 10,
batch_size = 13,
validation_split = 0.2,
verbose=2
)
?fit.keras.engine.training.Model
knitr::opts_chunk$set(echo = TRUE)
library(reticulate)
library(magrittr)
library(data.table)
library(NLP)
library(tm)
tweets_reduced_single_emoji <- fread("./data/reduced_tweets_single.csv")
tweets_165_single_emoji <- fread("./data/165tweets.csv")
tweetDT <- tweets_165_single_emoji
nrow(tweets_165_single_emoji)
tweetDT[sample(1:.N,3)]
tweetDT$text<-gsub('[^\x01-\x7F]',' ',tweetDT$text)
tweetDT$text<-gsub('[0-9]+',' ',tweetDT$text)
tweetDT$text<-gsub('\\b\\w{1,2}\\s','',tweetDT$text)
tweetDT$text<-gsub('\\s+',' ',tweetDT$text)
tweetDT$text<-gsub(' $','',tweetDT$text)
tweetDT$text<-tolower(tweetDT$text)
tweetDT[sample(1:.N,10)]
tweetDT$text %>% strsplit(" ") %>% sapply(length) %>% summary()
library(keras)
library(dplyr)
library(ggplot2)
library(purrr)
tweetDT_oh[sample(1:.N,10)]
training_id <- sample.int(nrow(tweetDT), size = nrow(tweetDT)*0.8)
training <- tweetDT[training_id,]
testing <- tweetDT[-training_id,]
nrow(training)
nrow(testing)
num_words <- 1000
max_length <- 50
text_vectorization <- layer_text_vectorization(
max_tokens = num_words,
output_sequence_length = max_length,
)
text_vectorization %>% adapt(tweetDT$text)
text_vectorization(matrix(tweetDT$text[1], ncol = 1))
emoji_vectorization(matrix(tweetDT$emoji[1], ncol = 1))
input <- layer_input(shape = c(1), dtype = "string")
output <- input %>%
text_vectorization() %>%
layer_embedding(input_dim = num_words + 1, output_dim = 16) %>%
layer_global_average_pooling_1d() %>%
layer_dense(units = 16, activation = "relu") %>%
layer_dropout(0.5) %>%
#emoji_vectorization() %>%
layer_dense(units = 50, activation = "softmax")
model <- keras_model(input, output)
model %>% compile(
optimizer = 'adam',
loss = 'catagorical_crossentropy',
metrics = list('accuracy')
)
history <- model %>% fit(
training$text,
emoji_vectorization(matrix(training$emoji)),
epochs = 10,
batch_size = 13,
validation_split = 0.2,
verbose=2
)
input <- layer_input(shape = c(1), dtype = "string")
output <- input %>%
text_vectorization() %>%
layer_embedding(input_dim = num_words + 1, output_dim = 16) %>%
layer_global_average_pooling_1d() %>%
layer_dense(units = 16, activation = "relu") %>%
layer_dropout(0.5) %>%
layer_dense(units = 50, activation = "softmax")%>%
emoji_vectorization()
?layer_text_vectorization
input <- layer_input(shape = c(1), dtype = "string")
output <- input %>%
text_vectorization() %>%
layer_embedding(input_dim = num_words + 1, output_dim = 16) %>%
layer_global_average_pooling_1d() %>%
layer_dense(units = 16, activation = "relu") %>%
layer_dropout(0.5) %>%
layer_dense(units = 50, activation = "softmax")
model <- keras_model(input, output)
model %>% compile(
optimizer = 'adam',
loss = 'catagorical_crossentropy',
metrics = list('accuracy')
)
history <- model %>% fit(
training$text,
training$emoji,
epochs = 10,
batch_size = 13,
validation_split = 0.2,
verbose=2
)
history <- model %>% fit(
training$text,
to_categorical(training$emoji),
epochs = 10,
batch_size = 13,
validation_split = 0.2,
verbose=2
)
