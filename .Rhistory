batch_size = 13,
validation_split = 0.2,
verbose=2
)
input <- layer_input(shape = c(1), dtype = "string")
output <- input %>%
text_vectorization() %>%
layer_embedding(input_dim = num_words + 1, output_dim = 16) %>%
layer_global_average_pooling_1d() %>%
layer_dense(units = 16, activation = "relu") %>%
layer_dropout(0.5) %>%
layer_dense(units = emoji_count, activation = "softmax")
model <- keras_model(input, output)
model %>% compile(
optimizer = 'adam',
loss = 'loss_catagorical_crossentropy',
metrics = list('accuracy')
)
history <- model %>% fit(
training$text,
oh_train,
epochs = 10,
batch_size = 13,
validation_split = 0.2,
verbose=2
)
model %>% compile(
optimizer = 'adam',
loss = 'categorical_crossentropy',
metrics = list('accuracy')
)
history <- model %>% fit(
training$text,
oh_train,
epochs = 10,
batch_size = 13,
validation_split = 0.2,
verbose=2
)
results <- model %>% evaluate(testing$text, oh_test, verbose = 0)
results
plot(history)
results <- model %>% evaluate(testing$text, oh_test, verbose = 0)
results
plot(history)
keras_predict(model,testing$text[1])
?predict_classes()
predict_classes(model,testing$text[1])
predict(model,testing$text[1])
predict(model,testing$text[1]) %>% plot()
prediction <- predict(model,testing$text[1])
prediction
View(prediction)
prediction <- predict(model,testing$text[1])
plot(prediction[1])
prediction <- predict(model,testing$text[1])
plot(prediction[,1])
prediction <- predict(model,testing$text[1])
plot(prediction[1,])
prediction <- predict(model,testing$text[1])[1,]
plot(prediction)
prediction <- predict(model,testing$text[1])[1,]
prediction
plot(prediction)
prediction <- predict(model,testing$text[1])[1,]
max(prediction)
plot(prediction)
prediction <- predict(model,testing$text[1])[1,]
which.max(prediction)
plot(prediction)
tweetDT[sample(1:.N,3)]
emoji_count <- unique(tweetDT$emoji)%>%length()
num_words <- 10000
max_length <- 50
text_vectorization <- layer_text_vectorization(
max_tokens = num_words,
output_sequence_length = max_length,
)
text_vectorization %>% adapt(tweetDT$text)
emoji_count <- unique(tweetDT$emoji)%>%length()
emoji_int <- match(tweetDT$emoji,unique(tweetDT$emoji))
one_hot_emoji <- to_categorical(emoji_int)
one_hot_emoji <- one_hot_emoji[,2:(emoji_count+1)]
training_id <- sample.int(nrow(tweetDT), size = nrow(tweetDT)*0.8)
training <- tweetDT[training_id,]
testing <- tweetDT[-training_id,]
oh_train <-one_hot_emoji[training_id,]
oh_test <- one_hot_emoji[-training_id,]
nrow(training)
nrow(oh_train)
nrow(testing)
nrow(oh_test)
emoji_count <- unique(tweetDT$emoji)%>%length()
emoji_count
training_id <- sample.int(nrow(tweetDT), size = nrow(tweetDT)*0.8)
training <- tweetDT[training_id,]
testing <- tweetDT[-training_id,]
oh_train <-one_hot_emoji[training_id,]
oh_test <- one_hot_emoji[-training_id,]
nrow(training)
nrow(oh_train)
nrow(testing)
nrow(oh_test)
input <- layer_input(shape = c(1), dtype = "string")
output <- input %>%
text_vectorization() %>%
layer_embedding(input_dim = num_words + 1, output_dim = 16) %>%
layer_global_average_pooling_1d() %>%
layer_dense(units = 16, activation = "relu") %>%
layer_dropout(0.5) %>%
layer_dense(units = emoji_count, activation = "softmax")
model <- keras_model(input, output)
model %>% compile(
optimizer = 'adam',
loss = 'categorical_crossentropy',
metrics = list('accuracy')
)
history <- model %>% fit(
training$text,
oh_train,
epochs = 10,
batch_size = 13,
validation_split = 0.2,
verbose=2
)
results <- model %>% evaluate(testing$text, oh_test, verbose = 0)
results
num_words <- 1000
max_length <- 50
text_vectorization <- layer_text_vectorization(
max_tokens = num_words,
output_sequence_length = max_length,
)
#text_vectorization %>% adapt(tweetDT$text)
num_words <- 1000
max_length <- 50
text_vectorization <- layer_text_vectorization(
max_tokens = num_words,
output_sequence_length = max_length,
)
text_vectorization %>% adapt(tweetDT$text)
num_words <- 1000
max_length <- 50
text_vectorization <- layer_text_vectorization(
max_tokens = num_words,
output_sequence_length = max_length,
)
#text_vectorization %>% adapt(tweetDT$text)
num_words <- 1000
max_length <- 50
text_vectorization <- layer_text_vectorization(
max_tokens = num_words,
output_sequence_length = max_length,
)
text_vectorization %>% adapt(tweetDT$text)
num_words <- 1000
max_length <- 50
text_vectorization <- layer_text_vectorization(
max_tokens = num_words,
output_sequence_length = max_length,
)
text_vectorization %>% adapt(tweetDT$text)
get_vocabulary(text_vectorization)%>%sample(10)
text_vectorization(matrix(tweetDT$text[1], ncol = 1))
tweetDT$text<-gsub('[^\x01-\x7F]',' ',tweetDT$text)
tweetDT$text<-gsub('[0-9]+',' ',tweetDT$text)
tweetDT$text<-gsub('\\b\\w{1,2}\\s','',tweetDT$text)
tweetDT$text<-gsub('\\s+',' ',tweetDT$text)
tweetDT$text<-gsub(' $','',tweetDT$text)
tweetDT$text<-tolower(tweetDT$text)
knitr::opts_chunk$set(echo = TRUE)
library(reticulate)
library(magrittr)
library(data.table)
library(NLP)
library(tm)
tweets_reduced_single_emoji <- fread("./data/reduced_tweets_single.csv")
tweets_165_single_emoji <- fread("./data/165tweets.csv")
tweetDT <- tweets_165_single_emoji
nrow(tweets_165_single_emoji)
tweetDT[sample(1:.N,3)]
tweetDT$text<-gsub('[^\x01-\x7F]',' ',tweetDT$text)
tweetDT$text<-gsub('[0-9]+',' ',tweetDT$text)
tweetDT$text<-gsub('\\b\\w{1,2}\\s','',tweetDT$text)
tweetDT$text<-gsub('\\s+',' ',tweetDT$text)
tweetDT$text<-gsub(' $','',tweetDT$text)
tweetDT$text<-tolower(tweetDT$text)
tweetDT[sample(1:.N,10)]
tweetDT$text %>% strsplit(" ") %>% sapply(length) %>% summary()
library(keras)
library(dplyr)
library(ggplot2)
library(purrr)
num_words <- 1000
max_length <- 50
text_vectorization <- layer_text_vectorization(
max_tokens = num_words,
output_sequence_length = max_length,
)
text_vectorization %>% adapt(tweetDT$text)
emoji_count <- unique(tweetDT$emoji)%>%length()
emoji_count
emoji_int <- match(tweetDT$emoji,unique(tweetDT$emoji))
one_hot_emoji <- to_categorical(emoji_int)
one_hot_emoji <- one_hot_emoji[,2:(emoji_count+1)]
training_id <- sample.int(nrow(tweetDT), size = nrow(tweetDT)*0.8)
training <- tweetDT[training_id,]
testing <- tweetDT[-training_id,]
oh_train <-one_hot_emoji[training_id,]
oh_test <- one_hot_emoji[-training_id,]
nrow(training)
nrow(oh_train)
nrow(testing)
nrow(oh_test)
input <- layer_input(shape = c(1), dtype = "string")
output <- input %>%
text_vectorization() %>%
layer_embedding(input_dim = num_words + 1, output_dim = 16) %>%
layer_global_average_pooling_1d() %>%
layer_dense(units = 16, activation = "relu") %>%
layer_dropout(0.5) %>%
layer_dense(units = emoji_count, activation = "softmax")
model <- keras_model(input, output)
model %>% compile(
optimizer = 'adam',
loss = 'categorical_crossentropy',
metrics = list('accuracy')
)
history <- model %>% fit(
training$text,
oh_train,
epochs = 10,
batch_size = 13,
validation_split = 0.2,
verbose=2
)
results <- model %>% evaluate(testing$text, oh_test, verbose = 0)
results
plot(history)
prediction_vector <- predict(model,testing$text[1])[1,]
predicted_int <- which.max(prediction)
predicted_emoji
tweets_10_single_emoji <- fread("./data/reduced_tweets_single.csv")
tweets_165_single_emoji <- fread("./data/165tweets.csv")
tweetDT <- tweets_165_single_emoji
nrow(tweets_165_single_emoji)
#tweets_10_single_emoji <- fread("./data/reduced_tweets_single.csv")
tweets_165_single_emoji <- fread("./data/165tweets.csv")
tweetDT <- tweets_165_single_emoji
nrow(tweets_165_single_emoji)
tweetDT[sample(1:.N,3)]
tweetDT$text<-gsub('[^\x01-\x7F]',' ',tweetDT$text)
tweetDT$text<-gsub('[0-9]+',' ',tweetDT$text)
tweetDT$text<-gsub('\\b\\w{1,2}\\s','',tweetDT$text)
tweetDT$text<-gsub('\\s+',' ',tweetDT$text)
tweetDT$text<-gsub(' $','',tweetDT$text)
tweetDT$text<-tolower(tweetDT$text)
View(tweetDT)
#tweets_10_single_emoji <- fread("./data/reduced_tweets_single.csv")
tweets_165_single_emoji <- fread("./data/165tweets.csv")
tweetDT <- tweets_165_single_emoji
nrow(tweets_165_single_emoji)
tweetDT$text<-gsub('[^\x01-\x7F]',' ',tweetDT$text)
tweetDT$text<-gsub('[0-9]+',' ',tweetDT$text)
tweetDT$text<-gsub('\\b\\w{1,3}\\s','',tweetDT$text)
tweetDT$text<-gsub('\\s+',' ',tweetDT$text)
tweetDT$text<-gsub(' $','',tweetDT$text)
tweetDT$text<-tolower(tweetDT$text)
tweetDT[sample(1:.N,10)]
corp = VCorpus(VectorSource(tweetDT$text))
writeLines(as.character(corp[[1]]))
corp <- tm_map(corp, removePunctuation)
corp <- tm_map(corp,content_transformer(tolower))
removeChar <- content_transformer(function(x, pattern) {return (gsub(pattern," ", x))})
docs <- tm_map(docs, removeChar,"")
corp = VCorpus(VectorSource(tweetDT$text))
writeLines(as.character(corp[[1]]))
corp <- tm_map(corp, removePunctuation)
corp <- tm_map(corp,content_transformer(tolower))
removeChar <- content_transformer(function(x, pattern) {return (gsub(pattern," ", x))})
corp <- tm_map(corp, removeChar,"%")
View(corp)
#tweets_10_single_emoji <- fread("./data/reduced_tweets_single.csv")
tweets_165_single_emoji <- fread("./data/165tweets.csv")
tweetDT <- tweets_165_single_emoji
nrow(tweets_165_single_emoji)
tweetDT$text<-gsub('[^\x01-\x7F]',' ',tweetDT$text)
tweetDT$text<-gsub('[0-9]+',' ',tweetDT$text)
tweetDT$text<-gsub('\\b\\w{1,3}\\s','',tweetDT$text)
tweetDT$text<-gsub('\\s+',' ',tweetDT$text)
tweetDT$text<-gsub(' $','',tweetDT$text)
tweetDT$text<-tolower(tweetDT$text)
tweetDT[sample(1:.N,10)]
#tweets_10_single_emoji <- fread("./data/reduced_tweets_single.csv")
tweets_165_single_emoji <- fread("./data/165tweets.csv")
tweetDT <- tweets_165_single_emoji
nrow(tweets_165_single_emoji)
tweetDT$text<-gsub('[^\x01-\x7F]',' ',tweetDT$text)
tweetDT$text<-gsub('[0-9]+',' ',tweetDT$text)
tweetDT$text<-gsub('\\b\\w{1,2}\\s','',tweetDT$text)
tweetDT$text<-gsub('\\s+',' ',tweetDT$text)
tweetDT$text<-gsub(' $','',tweetDT$text)
tweetDT$text<-tolower(tweetDT$text)
tweetDT[sample(1:.N,10)]
tweetDT$text<-gsub('[^\x01-\x7F]',' ',tweetDT$text)
tweetDT$text<-gsub('[0-9]+',' ',tweetDT$text)
tweetDT$text<-gsub('\\b\\w{1}\\s','',tweetDT$text)
tweetDT$text<-gsub('\\s+',' ',tweetDT$text)
tweetDT$text<-gsub(' $','',tweetDT$text)
tweetDT$text<-tolower(tweetDT$text)
#tweets_10_single_emoji <- fread("./data/reduced_tweets_single.csv")
tweets_165_single_emoji <- fread("./data/165tweets.csv")
tweetDT <- tweets_165_single_emoji
nrow(tweets_165_single_emoji)
tweetDT$text<-gsub('[^\x01-\x7F]',' ',tweetDT$text)
tweetDT$text<-gsub('[0-9]+',' ',tweetDT$text)
tweetDT$text<-gsub('\\b\\w{1}\\s','',tweetDT$text)
tweetDT$text<-gsub('\\s+',' ',tweetDT$text)
tweetDT$text<-gsub(' $','',tweetDT$text)
tweetDT$text<-tolower(tweetDT$text)
tweetDT[sample(1:.N,10)]
tweetDT$text %>% strsplit(" ") %>% sapply(length) %>% summary()
library(keras)
library(dplyr)
library(ggplot2)
library(purrr)
num_words <- 1000
max_length <- 50
text_vectorization <- layer_text_vectorization(
max_tokens = num_words,
output_sequence_length = max_length,
)
text_vectorization %>% adapt(tweetDT$text)
?layer_text_vectorization
emoji_count <- unique(tweetDT$emoji)%>%length()
emoji_count
emoji_int <- match(tweetDT$emoji,unique(tweetDT$emoji))
one_hot_emoji <- to_categorical(emoji_int)
one_hot_emoji <- one_hot_emoji[,2:(emoji_count+1)]
training_id <- sample.int(nrow(tweetDT), size = nrow(tweetDT)*0.8)
training <- tweetDT[training_id,]
testing <- tweetDT[-training_id,]
oh_train <-one_hot_emoji[training_id,]
oh_test <- one_hot_emoji[-training_id,]
nrow(training)
nrow(oh_train)
nrow(testing)
nrow(oh_test)
input <- layer_input(shape = c(1), dtype = "string")
output <- input %>%
text_vectorization() %>%
layer_embedding(input_dim = num_words + 1, output_dim = 16) %>%
layer_global_average_pooling_1d() %>%
layer_dense(units = 16, activation = "relu") %>%
layer_dropout(0.5) %>%
layer_dense(units = emoji_count, activation = "softmax")
model <- keras_model(input, output)
history <- model %>% fit(
training$text,
oh_train,
epochs = 10,
batch_size = 13,
validation_split = 0.2,
verbose=2
)
model %>% compile(
optimizer = 'adam',
loss = 'categorical_crossentropy',
metrics = list('accuracy')
)
history <- model %>% fit(
training$text,
oh_train,
epochs = 10,
batch_size = 13,
validation_split = 0.2,
verbose=2
)
results <- model %>% evaluate(testing$text, oh_test, verbose = 0)
results
plot(history)
prediction_vector <- predict(model,testing$text[1])[1,]
predicted_int <- which.max(prediction)
prediction <- predict(model,testing$text[1])[1,]
predicted_int <- which.max(prediction)
predicted_emoji
emoji_int <- match(tweetDT$emoji,unique(tweetDT$emoji))
emoji_int
one_hot_emoji <- to_categorical(emoji_int)
one_hot_emoji <- one_hot_emoji[,2:(emoji_count+1)]
emoji_int <- match(tweetDT$emoji,unique(tweetDT$emoji))
one_hot_emoji <- to_categorical(emoji_int)
one_hot_emoji <- one_hot_emoji[,2:(emoji_count+1)]
emoji_dict <- cbind(emoji_int,tweetDT$emoji)%>%unique()
emoji_dict
prediction <- predict(model,testing$text[1])[1,]
predicted_int <- which.max(prediction)
predicted_emoji <- emoji_dict[predicted_int]
predicted_emoji
View(emoji_dict)
emoji_count <- unique(tweetDT$emoji)%>%length()
emoji_count
emoji_int <- match(tweetDT$emoji,unique(tweetDT$emoji))
one_hot_emoji <- to_categorical(emoji_int)
one_hot_emoji <- one_hot_emoji[,2:(emoji_count+1)]
emoji_dict <- cbind(emoji_int,tweetDT$emoji as emoji)%>%unique()
emoji_count <- unique(tweetDT$emoji)%>%length()
emoji_count
emoji_int <- match(tweetDT$emoji,unique(tweetDT$emoji))
one_hot_emoji <- to_categorical(emoji_int)
one_hot_emoji <- one_hot_emoji[,2:(emoji_count+1)]
emoji_dict <- tweetDT$emoji%>%unique()
emoji_dict
prediction <- predict(model,testing$text[1])[1,]
predicted_int <- which.max(prediction)
predicted_emoji <- emoji_dict[predicted_int]
predicted_emoji
prediction <- predict(model,testing$text[1])[1,]
predicted_int <- which.max(prediction)
predicted_emoji <- emoji_dict[predicted_int]
print("predicted")
predicted_emoji
prediction <- predict(model,testing$text[1])[1,]
predicted_int <- which.max(prediction)
predicted_emoji <- emoji_dict[predicted_int]
print("predicted: " predicted_emoji)
prediction <- predict(model,testing$text[1])[1,]
predicted_int <- which.max(prediction)
predicted_emoji <- emoji_dict[predicted_int]
print("predicted: " +predicted_emoji)
prediction <- predict(model,testing$text[1])[1,]
predicted_int <- which.max(prediction)
predicted_emoji <- emoji_dict[predicted_int]
print("predicted: ~predicted_emoji")
prediction <- predict(model,testing$text[1])[1,]
predicted_int <- which.max(prediction)
predicted_emoji <- emoji_dict[predicted_int]
print("predicted: " ~predicted_emoji)
?print
prediction <- predict(model,testing$text[1])[1,]
predicted_int <- which.max(prediction)
predicted_emoji <- emoji_dict[predicted_int]
print("predicted: ",predicted_emoji)
prediction <- predict(model,testing$text[1])[1,]
predicted_int <- which.max(prediction)
predicted_emoji <- emoji_dict[predicted_int]
cat("predicted: ",predicted_emoji)
prediction <- predict(model,testing$text[1])[1,]
predicted_int <- which.max(prediction)
predicted_emoji <- emoji_dict[predicted_int]
cat("predicted:",predicted_emoji)
cat("true",testing$emoji[1])
prediction <- predict(model,testing$text[1])[1,]
predicted_int <- which.max(prediction)
predicted_emoji <- emoji_dict[predicted_int]
cat("predicted:",predicted_emoji)
cat("true",testing$emoji[1])
prediction <- predict(model,testing$text[1])[1,]
predicted_int <- which.max(prediction)
predicted_emoji <- emoji_dict[predicted_int]
cat("predicted:",predicted_emoji)
print(" ")
cat("true",testing$emoji[1])
prediction <- predict(model,testing$text[1])[1,]
predicted_int <- which.max(prediction)
predicted_emoji <- emoji_dict[predicted_int]
cat("predicted:",predicted_emoji)
print()
prediction <- predict(model,testing$text[1])[1,]
predicted_int <- which.max(prediction)
predicted_emoji <- emoji_dict[predicted_int]
cat("predicted:",predicted_emoji)
print("")
cat("true",testing$emoji[1])
prediction <- predict(model,testing$text[1])[1,]
predicted_int <- which.max(prediction)
predicted_emoji <- emoji_dict[predicted_int]
cat("predicted:",predicted_emoji)
print(" ")
cat("true",testing$emoji[1])
prediction <- predict(model,testing$text[1])[1,]
predicted_int <- which.max(prediction)
predicted_emoji <- emoji_dict[predicted_int]
cat("predicted:",predicted_emoji)
print("\n")
cat("true",testing$emoji[1])
prediction <- predict(model,testing$text[1])[1,]
predicted_int <- which.max(prediction)
predicted_emoji <- emoji_dict[predicted_int]
cat("predicted:",predicted_emoji)
print("true",testing$emoji[1])
prediction <- predict(model,testing$text[1])[1,]
predicted_int <- which.max(prediction)
predicted_emoji <- emoji_dict[predicted_int]
print(cat("predicted:",predicted_emoji))
print(cat("true",testing$emoji[1]))
prediction <- predict(model,testing$text[1])[1,]
predicted_int <- which.max(prediction)
predicted_emoji <- emoji_dict[predicted_int]
print(cat("predicted:",predicted_emoji),quote=FALSE)
print(cat("true",testing$emoji[1]),quote=FALSE)
prediction <- predict(model,testing$text[1])[1,]
predicted_int <- which.max(prediction)
predicted_emoji <- emoji_dict[predicted_int]
print(paste("predicted:",predicted_emoji))
print(paste("true",testing$emoji[1]))
prediction <- predict(model,testing$text[1])[1,]
predicted_int <- which.max(prediction)
predicted_emoji <- emoji_dict[predicted_int]
print(paste("predicted:",predicted_emoji))
print(paste("true",testing$emoji[1]))
print("this example happens to be correct but the accuracy is 18%")
