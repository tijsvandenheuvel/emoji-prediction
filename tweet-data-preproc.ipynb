{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "Python 3.8.1 64-bit",
   "display_name": "Python 3.8.1 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "082e9a3bcad0a290d0001e938aa60b99250c6c2ef33a923c00b70f9826caf4b7"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# data cleaning\n",
    "\n",
    "turn tweet data set into workable dataframe using pyspark"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "sc = SparkContext.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = sc.textFile(\"./data/10tweets.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_clean = tweets.flatMap(lambda x: x.split(' ')).filter(lambda x:x!='O').filter(lambda x:x!='').filter(lambda x:x!='<STOP>')\n"
   ]
  },
  {
   "source": [
    "this operation is super slow\n",
    "\n",
    "reducing the entire dataset to one string and then splitting it and re parallelizing \n",
    "\n",
    "can this be done while remaining an rdd ? "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_string = tweets_clean.reduce(lambda x,y:x+' '+y)\n",
    "import re\n",
    "res = re.compile('\\<START\\>').split(tweets_string)\n",
    "res = list(filter(None, res))\n",
    "tweet_rdd = sc.parallelize(res)"
   ]
  },
  {
   "source": [
    "group text to 1 tweet in rdd\n",
    "\n",
    "find way to add key to each element\n",
    "\n",
    "counter doesnt work in rdd "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = tweet_rdd.zipWithIndex().map(lambda x: (x[1],x[0]))"
   ]
  },
  {
   "source": [
    "### create data table compatible structure\n",
    "\n",
    "- id , text , emojis\n",
    "\n",
    "- tab seperate multiple emojis in one tweet"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get emojis out of text\n",
    "emoji_rdd = tweets.map(lambda x: (x[0],re.findall(\":.*?:\",x[1])))\n",
    "emoji_rdd_tab_sep = emoji_rdd.map(lambda x:(x[0],'\\t'.join(x[1])))\n",
    "\n",
    "text_rdd = tweets.map(lambda x: (x[0],re.sub(\":.*?:\",\"\",x[1])))\n",
    "\n",
    "df_rdd1 = text_rdd.leftOuterJoin(emoji_rdd_tab_sep).map(lambda x:(x[0],x[1][0],x[1][1]))"
   ]
  },
  {
   "source": [
    "### duplicate tweets with multiple emojis into single emoji tweets"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "emoji_rdd1 = emoji_rdd.map(lambda x:(x[0],x[1][0]))\n",
    "max_emoji = emoji_rdd.map(lambda x: len(x[1])).max()\n",
    "\n",
    "for i in range(1,max_emoji):\n",
    "    emoji_rdd2 = emoji_rdd.filter(lambda x:len(x[1])>i).map(lambda x: (x[0],x[1][i]))\n",
    "    if i==1:\n",
    "        emoji_rdd3 = emoji_rdd1.union(emoji_rdd2)\n",
    "    else:\n",
    "        emoji_rdd3 = emoji_rdd3.union(emoji_rdd2)\n",
    "\n",
    "df_rdd2 = text_rdd.leftOuterJoin(emoji_rdd3).map(lambda x:(x[0],x[1][0],x[1][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SQLContext\n",
    "\n",
    "s   = SQLContext(sc)\n",
    "df = s.createDataFrame(df_rdd2, ['id', 'text','emoji'])\n",
    "\n",
    "df.repartition(1).write.csv('./data/outv4', sep=',', header=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}