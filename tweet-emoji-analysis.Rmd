---
title: "Tweet analysis"
author: "Tijs Van den Heuvel"
date: "10/15/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(reticulate)
library(magrittr)
library(data.table)
library(NLP)
library(tm)
```

# Tweet sentiment analysis and emoji prediction

The purpose of this project is to try and predict an emoji based on text.

It would be great if an algorithm could suggest some emojis based on a different piece of text.

The data set consists of a collection of tweets with at least one emoji.

[data source](https://www.kaggle.com/rexhaif/emojifydata-en)

The data is gathered, selected and reformatted by _Daniil Larionov_ from the _ArchiveTeam Twitter data_ source

I transformed this data to turn it into a data table in this jupyter notebook: 
[preprocess](tweet-pre-process.ipynb)
with the use of PySpark and regex
- this transformation process took a lot of work to make fast enough for large data sets
- so I did it for 10 and 165 tweets to get started 
- I did some initial experimentation and data cleaning with the 10 tweets set
- then I tried to create and run a model with the 165 tweets set
- when that model worked (with an accuracy of 18%) I proceeded to fix the pre-processor & get the entire data set

Full data set 
- already split into training and test set
- unpractical for cleansing and one hot encoding of output
```{r eval = FALSE}
trainDT <- fread("./data/full_train.csv")
testDT <- fread("./data/tweets_dev.csv")
tweetDT=rbind(trainDT,testDT)
```
Develoment data set
```{r eval = TRUE}
#tweetDT <- fread("./data/165tweets.csv")
tweetDT <- fread("./data/tweets_dev.csv")

#nrow(tweetDT)
```

## a sample of the tweets

```{r}
tweetDT[sample(1:.N,3)]
```

## clean text

- remove all non ascii characters
- remove numbers 
- (remove words of length 1 )
- merge multiple spaces
- remove trailing spaces
- to lower case
- remove duplicate rows
- remove empty tweets

> some of these steps could remove too much information, can be revised
> also further cleaning can be done


```{r}
nrow(tweetDT)

tweetDT$text<-gsub('[^\x01-\x7F]',' ',tweetDT$text)
tweetDT$text<-gsub('[0-9]+',' ',tweetDT$text)
#tweetDT$text<-gsub('\\b\\w{1}\\s','',tweetDT$text)
tweetDT$text<-gsub('\\s+',' ',tweetDT$text)
tweetDT$text<-gsub(' $','',tweetDT$text)
tweetDT$text<-tolower(tweetDT$text)

tweetDT<- tweetDT[!duplicated(tweetDT),]
tweetDT<- tweetDT[tweetDT$text!='',]

nrow(tweetDT)
```

```{r}
tweetDT[sample(1:.N,10)]
```

## convert to Corpus to use tm package
- text cleaning can be done with regex in data table
- I could use the tm package if I would want to analyse the data in a different way
```{r eval=FALSE}
corp = VCorpus(VectorSource(tweetDT$text))
writeLines(as.character(corp[[1]]))
corp <- tm_map(corp, removePunctuation)
corp <- tm_map(corp,content_transformer(tolower))
removeChar <- content_transformer(function(x, pattern) {return (gsub(pattern," ", x))})
corp <- tm_map(corp, removeChar,"%")
```

## distribution of number of words in a tweet 

> maybe calculate amount of unique words 

```{r}
tweetDT$text %>% strsplit(" ") %>% sapply(length) %>% summary()
```
```{r}
tweetDT$text %>% strsplit(" ") %>% sapply(length) %>%plot()
```

## how many different emoji's are present
- this will determine the size of the output layer of the network
```{r}
emoji_count <- unique(tweetDT$emoji)%>%length()
emoji_count
```

## Text classification with Tensorflow for R

- here I start with some tensorflow tutorial
  - [tutorial](https://tensorflow.rstudio.com/tutorials/beginners/basic-ml/tutorial_basic_text_classification/)
- the input is the text of the tweet
- the output should be the emoji
- a neural network takes in tensors (matrix) and not text so I needed to prepare the data

### imports
```{r}
library(keras)
library(dplyr)
library(ggplot2)
library(purrr)
```

### prepare the input data

- the tweets must be converted to tensors to use in NN
- create a dictionary & represent each of the 10,000 most common words by an integer
- you could create a very sparse 1,0000D vector but that's too memory intensive (one hot)
- Tokenization is also a method for encoding words with a vocabulary index based on word frequency
- solution from tutorial:
  - pad the arrays so they all have same length
  - create integer tensor of shape `num_words * max_length`
  - use an embedding layer capable of handling this shape as first layer
- `adapt` the text vectorization layer to learn about the words in our data set

```{r}
num_words <- 10000
max_length <- 40
text_vectorization <- layer_text_vectorization(
  max_tokens = num_words, 
  output_sequence_length = max_length, 
)
text_vectorization %>% adapt(tweetDT$text)
```
#### text vectorization example / info 
show vocabulary 
```{r eval=FALSE}
get_vocabulary(text_vectorization)%>%sample(10)
```

show how text vectorization layer transforms input
```{r eval=FALSE}
text_vectorization(matrix(tweetDT$text[1], ncol = 1))
```

### prepare the output data

- I searched a long time how to encode the emojis
- the emojis have to be encoded as a vector
- one hot is a good method 
- keras had built in function: to_categorical but it needs integer input
- convert labels to integers
- first column is entirely empty so I remove it
- how many emojis are there -> size of final layer
- create an emoji dictionary to find which number maps to which emoji after prediction

```{r}
emoji_int <- match(tweetDT$emoji,unique(tweetDT$emoji))

one_hot_emoji <- to_categorical(emoji_int)
one_hot_emoji <- one_hot_emoji[,2:(emoji_count+1)]

emoji_dict <- tweetDT$emoji%>%unique()
```

### split the dataset into training & testing

- 80% training, 20% testing, arbitrarily chosen

```{r}
training_id <- sample.int(nrow(tweetDT), size = nrow(tweetDT)*0.8)
training <- tweetDT[training_id,]
testing <- tweetDT[-training_id,]
oh_train <-one_hot_emoji[training_id,]
oh_test <- one_hot_emoji[-training_id,]
nrow(training)
nrow(oh_train)
nrow(testing)
nrow(oh_test)
```

### build the model

- how many layers?
- how many hidden units?
- what types of building blocks?
- what type of architecture?

#### model 1 (0.29)
- just try something from the tutorial

- accuracy on the dev test set of 29%
- loss of 2.783
```{r eval = FALSE}
input <- layer_input(shape = c(1), dtype = "string")

output <- input %>% 
  text_vectorization() %>% 
  layer_embedding(input_dim = num_words + 1, output_dim = 16) %>%
  layer_global_average_pooling_1d() %>%
  layer_dense(units = 16, activation = "relu") %>%
  layer_dropout(0.5) %>% 
  layer_dense(units = emoji_count, activation = "softmax")

model <- keras_model(input, output)
```

#### model 2 (0.33)
- add an extra layer

- accuracy on the dev test set is 33%
- loss is 2.6548331 
```{r eval = FALSE}
input <- layer_input(shape = c(1), dtype = "string")

output <- input %>% 
  text_vectorization() %>% 
  layer_embedding(input_dim = num_words + 1, output_dim = 16) %>%
  layer_global_average_pooling_1d() %>%
  layer_dense(units = 16, activation = "relu") %>%
  layer_dense(units = 16, activation = "relu") %>%
  layer_dropout(0.2) %>% 
  layer_dense(units = emoji_count, activation = "softmax")

model <- keras_model(input, output)
```

#### model 3 (0.34)
- add an extra layer
- 2 dropout layers
- double the size of the layers: 16 is small to go from 50 tot 50
- the more complex network seems only slightly more performant

- accuracy on the dev test set is 0.34
- loss is 2.5792720
```{r eval = FALSE}
input <- layer_input(shape = c(1), dtype = "string")

output <- input %>% 
  text_vectorization() %>% 
  layer_embedding(input_dim = num_words + 1, output_dim = 16) %>%
  layer_global_average_pooling_1d() %>%
  layer_dense(units = 32, activation = "relu") %>%
  layer_dropout(0.1) %>% 
  layer_dense(units = 32, activation = "relu") %>%
  layer_dropout(0.1) %>% 
  layer_dense(units = 32, activation = "relu") %>%
  layer_dense(units = emoji_count, activation = "softmax")

model <- keras_model(input, output)
```

#### model 4 (0.36)
-the input and output are both size 50 so let's make the middle layers also size 50
- 2 dense layers
- 1 small dropout 

- accuracy on the dev test set is 0.3578
- accuracy on the full set is 0.36157 with loss 2.4654262 
- loss is 2.5344746
```{r eval = FALSE}
input <- layer_input(shape = c(1), dtype = "string")

output <- input %>% 
  text_vectorization() %>% 
  layer_embedding(input_dim = num_words + 1, output_dim = 16) %>%
  layer_global_average_pooling_1d() %>%
  layer_dense(units = 50, activation = "relu") %>%
  layer_dense(units = 50, activation = "relu") %>%
  layer_dropout(0.2) %>% 
  layer_dense(units = emoji_count, activation = "softmax")

model <- keras_model(input, output)
```

#### model 5 (0.34)
- 50 is perhaps a little big
- lets try a deeper more slender model

- accuracy on the dev test set is 0.3398407 
- loss is 2.6135445 
```{r eval = FALSE}
input <- layer_input(shape = c(1), dtype = "string")

output <- input %>% 
  text_vectorization() %>% 
  layer_embedding(input_dim = num_words + 1, output_dim = 16) %>%
  layer_global_average_pooling_1d() %>%
  layer_dense(units = 32, activation = "relu") %>%
  layer_dense(units = 32, activation = "relu") %>%
  layer_dropout(0.1) %>% 
  layer_dense(units = 32, activation = "relu") %>%
  layer_dropout(0.1) %>% 
  layer_dense(units = 32, activation = "relu") %>%
  layer_dropout(0.1) %>% 
  layer_dense(units = emoji_count, activation = "softmax")

model <- keras_model(input, output)
```



#### model 6 (0.36)
- three layers of 40 each 

- took 3 hours to train on the full train set
- accuracy on the full set is 0.3630368 
- with loss 2.4692233 
```{r eval = FALSE}
input <- layer_input(shape = c(1), dtype = "string")

output <- input %>% 
  text_vectorization() %>% 
  layer_embedding(input_dim = num_words + 1, output_dim = 16) %>%
  layer_global_average_pooling_1d() %>%
  layer_dense(units = 40, activation = "relu") %>%
  layer_dense(units = 40, activation = "relu") %>%
  layer_dense(units = 40, activation = "relu") %>%
  layer_dropout(0.2) %>% 
  layer_dense(units = emoji_count, activation = "softmax")

model <- keras_model(input, output)
```

#### model 7 (0.377)

- modify the layer embedding from 16 to 40

- text vectorization with max length 100
  - perf on dev set loss: 2.6350586 accuracy: 0.3726049
  - run time 47 minutes
  
- text vectorization with max length 50
  - perf on dev set loss: 2.6026843  accuracy: 0.3773668 
  - run time 44 minutes
```{r eval = FALSE}
input <- layer_input(shape = c(1), dtype = "string")

output <- input %>% 
  text_vectorization() %>% 
  layer_embedding(input_dim = num_words + 1, output_dim = 40) %>%
  layer_global_average_pooling_1d() %>%
  layer_dense(units = 40, activation = "relu") %>%
  layer_dense(units = 40, activation = "relu") %>%
  layer_dense(units = 40, activation = "relu") %>%
  layer_dropout(0.2) %>% 
  layer_dense(units = emoji_count, activation = "softmax")

model <- keras_model(input, output)
```

#### model 8 (0.362)

- same amount of nodes but distributed over more layers is slower and worse

- perf on dev set loss: 2.6350629    accuracy: 0.3620144
- run time 1.05 hours
```{r eval = FALSE}
input <- layer_input(shape = c(1), dtype = "string")

output <- input %>% 
  text_vectorization() %>% 
  layer_embedding(input_dim = num_words + 1, output_dim = 40) %>%
  layer_global_average_pooling_1d() %>%
  layer_dense(units = 30, activation = "relu") %>%
  layer_dense(units = 30, activation = "relu") %>%
  layer_dense(units = 30, activation = "relu") %>%
  layer_dense(units = 30, activation = "relu") %>%
  layer_dropout(0.2) %>% 
  layer_dense(units = emoji_count, activation = "softmax")

model <- keras_model(input, output)
```

#### model 9 (0.387)

- 4 x 50 nodes

- perf on dev set loss: 2.6434860 accuracy:  0.3869762
- the val_loss was rising, this is a sign of over fitting

```{r eval=FALSE}
input <- layer_input(shape = c(1), dtype = "string")

output <- input %>% 
  text_vectorization() %>% 
  layer_embedding(input_dim = num_words + 1, output_dim = 50) %>%
  layer_global_average_pooling_1d() %>%
  layer_dense(units = 50, activation = "relu") %>%
  layer_dense(units = 50, activation = "relu") %>%
  layer_dense(units = 50, activation = "relu") %>%
  layer_dropout(0.2) %>% 
  layer_dense(units = emoji_count, activation = "softmax")

model <- keras_model(input, output)
```

#### model 10 (0.373)

- 4 x 50 nodes
- double dropout (0.2-> 0.4) because last seemed to over fit (val_loss was increasing)
- change epochs from 100-50 because it didn't do much it the later epochs
- this is a little less performing than 2.3 but trains twice as fast and show less signs of over fitting

- perf on dev set loss: 2.6168747 accuracy:  0.3732527
- run time 26 minutes
```{r eval = FALSE}
input <- layer_input(shape = c(1), dtype = "string")

output <- input %>% 
  text_vectorization() %>% 
  layer_embedding(input_dim = num_words + 1, output_dim = 50) %>%
  layer_global_average_pooling_1d() %>%
  layer_dense(units = 50, activation = "relu") %>%
  layer_dense(units = 50, activation = "relu") %>%
  layer_dense(units = 50, activation = "relu") %>%
  layer_dropout(0.4) %>% 
  layer_dense(units = emoji_count, activation = "softmax")

model <- keras_model(input, output)
```

#### model 11 (0.374)

- 5 x 50 nodes
- add an extra layer 
- nothing changes
- this model is larger so seems to need more epochs: train for another 50 epochs
- more epoch had no significant impact on performance, the loss is even higher

50 epochs 
- perf on dev set loss: 2.6722088  accuracy: 0.3744027
- run time 24 minutes 

100 epochs 
- perf on dev set loss: 2.859356  accuracy: 0.379285 
- run time another 24 minutes 


```{r eval = FALSE}
input <- layer_input(shape = c(1), dtype = "string")

output <- input %>% 
  text_vectorization() %>% 
  layer_embedding(input_dim = num_words + 1, output_dim = 50) %>%
  layer_global_average_pooling_1d() %>%
  layer_dense(units = 50, activation = "relu") %>%
  layer_dense(units = 50, activation = "relu") %>%
  layer_dense(units = 50, activation = "relu") %>%
  layer_dropout(0.4) %>% 
  layer_dense(units = emoji_count, activation = "softmax")

model <- keras_model(input, output)
```

#### model 2.1 (0.251)

- 50 epoch 
- full set
- more dropout against over fitting
- cleaner data, no duplicates, no empty tweets

36 epochs on full set(4.3 mil) on GPU
- (50 words per tweet)
- acc:    0.250
- loss:   2.8899
- time:   40 min

50 epochs (couldnt stop sooner) on full set(4.3 mil) on GPU 
- (40 words per tweet)
- acc:    0.251
- loss:   2.8873570 
- time:   47.28701
```{r eval=FALSE}
input <- layer_input(shape = c(1), dtype = "string")

output <- input %>% 
  text_vectorization() %>% 
  layer_embedding(input_dim = num_words + 1, output_dim = 50) %>%
  layer_global_average_pooling_1d() %>%
  layer_dense(units = 50, activation = "relu") %>%
  layer_dropout(0.2) %>% 
  layer_dense(units = 50, activation = "relu") %>%
  layer_dropout(0.2) %>% 
  layer_dense(units = 50, activation = "relu") %>%
  layer_dropout(0.2) %>% 
  layer_dense(units = emoji_count, activation = "softmax")

model <- keras_model(input, output)
```
#### model 2.2 (0.254)

- 50 epoch 
- full set
- more dropout against over fitting
- cleaner data, no duplicates, no empty tweets

- 40 words per tweet
- larger input dimensions

won some accuracy but overfitted from epoch 20

26 epochs on full set(4.3 mil) on GPU 
- acc:    0.2541756
- loss:   2.8667824 
- time:   34 min
```{r eval=FALSE}
input <- layer_input(shape = c(1), dtype = "string")

output <- input %>% 
  text_vectorization() %>% 
  layer_embedding(input_dim = num_words + 1, output_dim = 100) %>%
  layer_global_average_pooling_1d() %>%
  layer_dense(units = 80, activation = "relu") %>%
  layer_dropout(0.2) %>% 
  layer_dense(units = 60, activation = "relu") %>%
  layer_dropout(0.2) %>% 
  layer_dense(units = 50, activation = "relu") %>%
  layer_dropout(0.2) %>% 
  layer_dense(units = emoji_count, activation = "softmax")

model <- keras_model(input, output)
```
#### model 2.2B

- 50 epoch 
- full set
- more dropout against over fitting
- cleaner data, no duplicates, no empty tweets

- 40 words per tweet
- smaller input dimensions


 epochs on full set(4.3 mil) on GPU 
- acc:    
- loss:   
- time:   
```{r eval =FALSE}
input <- layer_input(shape = c(1), dtype = "string")

output <- input %>% 
  text_vectorization() %>% 
  layer_embedding(input_dim = num_words + 1, output_dim = 40) %>%
  layer_global_average_pooling_1d() %>%
  layer_dense(units = 42, activation = "relu") %>%
  layer_dropout(0.2) %>% 
  layer_dense(units = 44, activation = "relu") %>%
  layer_dropout(0.2) %>% 
  layer_dense(units = 46, activation = "relu") %>%
  layer_dropout(0.2) %>% 
  layer_dense(units = emoji_count, activation = "softmax")

model <- keras_model(input, output)
```
#### model 3.1 LSTM

- 40 words per tweet
- LSTM : good for long term memory, but very slow and 
- the tweets are mostly just one sentence so try just RNN

batch size 300 takes 360 seconds per epoch on CPU

2 lstm layers
 epochs on dev set on GPU 
- 28 epochs to overfit
- acc:    0.2447176
- loss:   2.9154146 
- time:   2.5 hours
```{r eval=FALSE}
input <- layer_input(shape = c(1), dtype = "string")

output <- input %>% 
  text_vectorization() %>% 
  layer_embedding(input_dim = num_words + 1, output_dim = 40) %>%
  layer_spatial_dropout_1d(0.3)%>%
  layer_lstm(units = 45,dropout=0.5, recurrent_dropout=0.5,return_sequences = TRUE)%>%
  layer_lstm(units = 45,dropout=0.5, recurrent_dropout=0.5)%>%
  layer_dense(units = emoji_count, activation = "softmax")

model <- keras_model(input, output)
```
#### model 3.2 simple RNN

- 40 words per tweet
- LSTM : good for long term memory, but very slow
- the tweets are mostly just one sentence so try just RNN

determine batch size for cpu
- batch size 128 takes 130 sec/epoch
- batch size 256 takes 120 sec/epoch
- batch size 512 takes 100 sec/epoch 
- batch size 1024 takes 80 sec/epoch (loss=nan)

I encounter loss = nan
- could be caused by
  - large layers
  - large batch size
  - bad input (too much zeros in input vector) 
    - check input data
    
- with one simple rnn layer, the val_accuracy doenst increase
- added second simple rnn layer
  - 100s -> 160s/epoch
  - val accuracy stays exactly the same
  
- simple rnn is too weak an architecture to learn the patterns in the data ?

```{r eval =FALSE}
input <- layer_input(shape = c(1), dtype = "string")

output <- input %>% 
  text_vectorization() %>% 
  layer_embedding(input_dim = num_words + 1, output_dim = 40) %>%
  layer_spatial_dropout_1d(0.2)%>%
  layer_simple_rnn(units = 45,activation = "relu",dropout=0.2, 
                   recurrent_dropout=0.2,return_sequences = TRUE)%>%
  layer_simple_rnn(units = 45,activation = "relu",dropout=0.2, recurrent_dropout=0.2)%>%
  layer_dense(units = emoji_count, activation = "softmax")

model <- keras_model(input, output)
```

#### model 3.3 GRU

[GRU explain](https://blog.floydhub.com/gru-with-pytorch/)

- 40 words per tweet
- LSTM : good for long term memory, but very slow
- RNN doesnt learn
- GRU are less complex but similar to LSTM

1 GRU layer
- 230 seconds per epoch (100/140 seconds on GPU)
- with only one layer we have overfitting in 7 epochs (30 mins)
- val_acc of 0.247

2 GRU layers
- 360 seconds per epoch cpu
- 250 sec/epoch GPU
- also overfitting in 7 epochs, same val

battle overfitting by raising dropout from 0.2 to 0.3
- overfits after 9 epochs 
raising dropout from 0.3 to 0.4
- overfits after 15 epochs 
- accuracy 0.2500655
- loss 2.8953950

when using GPU with dropout inside gru layer: 140s/epoch
GPU with dropout outside gru layer : 100 s/epoch (overfit in 10 epochs)
- dropout inside is not compatible with cudnn library
- faster but no recurrent_dropout

2 GRU layers with 0.5 dropout + layer spatial dropout 0.2->0.3 dropout
GPU but no CUDNN
- overfit after 30 epochs (2 hours)
- loss 2.9230042
- acc  0.2446868 

3 GRU layers
- 360 seconds per epoch, is just too slow 

> 2 layer CUDNN doenst work, fix & improve regularization


```{r}
input <- layer_input(shape = c(1), dtype = "string")

output <- input %>% 
  text_vectorization() %>% 
  layer_embedding(input_dim = num_words + 1, output_dim = 40) %>%
  layer_spatial_dropout_1d(0.3)%>%
  layer_gru(units = 45,dropout=0.5, recurrent_dropout=0.5,return_sequences = TRUE)%>%
  layer_gru(units = 45,dropout=0.5, recurrent_dropout=0.5)%>%
  #layer_gru(units = 45,return_state = TRUE)%>%layer_dropout(0.5)%>%
  layer_dense(units = emoji_count, activation = "softmax")

model <- keras_model(input, output)
```


### compile model with loss function and optimizer
the tutorial uses binary_crossentropy but our variables are categorical
```{r}
model %>% compile(
  optimizer = 'adam',
  loss = 'categorical_crossentropy',
  metrics = list('accuracy')
)
```

### Train the model

```{r}
t1<-Sys.time()
```

```{r}
history <- model %>% fit(
  training$text,
  oh_train,
  epochs = 50,
  batch_size = 1660, 
 #batch_size = 512, #cpu  
  validation_split = 0.2,
  verbose=2
) 
```

```{r}
time <- Sys.time()-t1
time
```

### evaluate model
```{r}
results <- model %>% evaluate(testing$text, oh_test, verbose = 0)
results
```

```{r eval=FALSE}
plot(history)
```

### single prediction

```{r}
id <- 9
amount <-10

prediction <- predict(model,testing$text[id])[1,]


top_val <- prediction[order(prediction)]%>%rev() %>%head(amount) %>% round(3)
#top_val

top_ids <- order(prediction) %>% rev() %>% head(amount)
#top_ids

top_emoji <- emoji_dict[top_ids]
top_prediction <- cbind(top_emoji,top_val)


print(paste("text:",testing$text[id]))
print(paste("true:",testing$emoji[id]))

print(top_prediction)


```

```{r eval = FALSE}

id <-10
prediction <- predict(model,testing$text[id])[1,]
predicted_int <- which.max(prediction)
predicted_emoji <- emoji_dict[predicted_int]

print(paste("predicted:",predicted_emoji))

print(paste("true:",testing$emoji[id]))


```

```{r eval = FALSE}
plot(prediction)
```

```{r eval = FALSE}
model %>% serialize_model() %>% save_model_tf("my_model")
```

```{r}
list.files("saved_model.pb")
new_model <- load_model_tf("model")
summary(new_model)
```
```{r}
new_model <- load_model_hdf5("my_model.h5")
summary(new_model)
```



## lets try some different things

- convolutional networks
- find similar projects on github
- [LSTM example in python](https://medium.com/@djajafer/multi-class-text-classification-with-keras-and-lstm-4c5525bef592)
- training on the full data set takes 3 hours so I will use the dev for development
- go back to creating cleaner dataset, lemmatization, remove nonsense, check if that gives improvement
- word embedding with ngrams








