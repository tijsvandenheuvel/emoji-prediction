---
title: "Tweet analysis"
author: "Tijs Van den Heuvel"
date: "10/15/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(reticulate)
library(magrittr)
library(data.table)
library(NLP)
library(tm)
```

# Tweet sentiment analysis and emoji prediction

The purpose of this project is to try and predict an emoji based on text.

It would be great if an algorithm could suggest some emojis based on a different piece of text.

The data set consists of a collection of tweets with at least one emoji.

[data source](https://www.kaggle.com/rexhaif/emojifydata-en)

The data is gathered, selected and reformatted by _Daniil Larionov_ from the _ArchiveTeam Twitter data_ source

I transformed this data to turn it into a data table in this jupyter notebook: 
[preprocess](tweet-pre-process.ipynb)
with the use of PySpark and regex
- this transformation process took a lot of work to make fast enough for large data sets
- so I did it for 10 and 165 tweets to get started 
- I did some initial experimentation and data cleaning with the 10 tweets set
- then I tried to create and run a model with the 165 tweets set
- when that model worked (with an accuracy of 18%) I proceeded to fix the pre-processor & get the entire data set

Full data set 
- already split into training and test set
- unpractical for cleansing and one hot encoding of output
```{r eval = FALSE}
trainDT <- fread("./data/full_train.csv")
testDT <- fread("./data/tweets_dev.csv")
tweetDT=rbind(trainDT,testDT)
```
Develoment data set
```{r}
#tweetDT <- fread("./data/165tweets.csv")
tweetDT <- fread("./data/tweets_dev.csv")

#nrow(tweetDT)
```

## a sample of the tweets

```{r}
tweetDT[sample(1:.N,3)]
```

## clean text

- remove all non ascii characters
- remove numbers 
- remove words of length 1 
- merge multiple spaces
- remove trailing spaces
- to lower case

> some of these steps could remove too much information, can be revised
> also further cleaning can be done

> there appear to be words of size 0, fix?

> this regex could be done in fewer steps, will this be faster?

```{r}
tweetDT$text<-gsub('[^\x01-\x7F]',' ',tweetDT$text)
tweetDT$text<-gsub('[0-9]+',' ',tweetDT$text)
tweetDT$text<-gsub('\\b\\w{1}\\s','',tweetDT$text)
tweetDT$text<-gsub('\\s+',' ',tweetDT$text)
tweetDT$text<-gsub(' $','',tweetDT$text)
tweetDT$text<-tolower(tweetDT$text)

```

```{r}
tweetDT[sample(1:.N,10)]
```

## convert to Corpus to use tm package
- text cleaning can be done with regex in data table
- I could use the tm package if I would want to analyse the data in a different way
```{r eval=FALSE}
corp = VCorpus(VectorSource(tweetDT$text))
writeLines(as.character(corp[[1]]))
corp <- tm_map(corp, removePunctuation)
corp <- tm_map(corp,content_transformer(tolower))
removeChar <- content_transformer(function(x, pattern) {return (gsub(pattern," ", x))})
corp <- tm_map(corp, removeChar,"%")
```

## distribution of number of words in a tweet 

> maybe calculate amount of unique words 

```{r}
tweetDT$text %>% strsplit(" ") %>% sapply(length) %>% summary()
```

## how many different emoji's are present
- this will determine the size of the output layer of the network
```{r}
emoji_count <- unique(tweetDT$emoji)%>%length()
emoji_count
```

## Text classification with Tensorflow for R

- here I start with some tensorflow tutorial
  - [tutorial](https://tensorflow.rstudio.com/tutorials/beginners/basic-ml/tutorial_basic_text_classification/)
- the input is the text of the tweet
- the output should be the emoji
- a neural network takes in tensors (matrix) and not text so I needed to prepare the data

### imports
```{r}
library(keras)
library(dplyr)
library(ggplot2)
library(purrr)
```

### prepare the input data

- the tweets must be converted to tensors to use in NN
- create a dictionary & represent each of the 10,000 most common words by an integer
- you could create a very sparse 1,0000D vector but that's too memory intensive (one hot)
- Tokenization is also a method for encoding words with a vocabulary index based on word frequency
- solution from tutorial:
  - pad the arrays so they all have same length
  - create integer tensor of shape `num_examples * max_length`
  - use an embedding layer capable of handling this shape as first layer
- `adapt` the text vectorization layer to learn about the words in our data set

```{r}
num_words <- 10000
max_length <- 50
text_vectorization <- layer_text_vectorization(
  max_tokens = num_words, 
  output_sequence_length = max_length, 
)
text_vectorization %>% adapt(tweetDT$text)
```

show vocabulary 
```{r eval=FALSE}
get_vocabulary(text_vectorization)%>%sample(10)
```

show how text vectorization layer transforms input
```{r eval=FALSE}
text_vectorization(matrix(tweetDT$text[1], ncol = 1))
```

### prepare the output data

- I searched a long time how to encode the emojis
- the emojis have to be encoded as a vector
- one hot is a good method 
- keras had built in function: to_categorical but it needs integer input
- convert labels to integers
- first column is entirely empty so I remove it
- how many emojis are there -> size of final layer
- create an emoji dictionary to find which number maps to which emoji after prediction

```{r}
emoji_int <- match(tweetDT$emoji,unique(tweetDT$emoji))

one_hot_emoji <- to_categorical(emoji_int)
one_hot_emoji <- one_hot_emoji[,2:(emoji_count+1)]

emoji_dict <- tweetDT$emoji%>%unique()
```

### split the dataset into training & testing

- 80% training, 20% testing, arbitrarily chosen

```{r}
training_id <- sample.int(nrow(tweetDT), size = nrow(tweetDT)*0.8)
training <- tweetDT[training_id,]
testing <- tweetDT[-training_id,]
oh_train <-one_hot_emoji[training_id,]
oh_test <- one_hot_emoji[-training_id,]
nrow(training)
nrow(oh_train)
nrow(testing)
nrow(oh_test)
```

### build the model

- how many layers?
- how many hidden units?
- what types of building blocks?
- what type of architecture?

#### model 1 (0.29)
- just try something from the tutorial

- accuracy on the dev test set of 29%
- loss of 2.783
```{r eval = FALSE}
input <- layer_input(shape = c(1), dtype = "string")

output <- input %>% 
  text_vectorization() %>% 
  layer_embedding(input_dim = num_words + 1, output_dim = 16) %>%
  layer_global_average_pooling_1d() %>%
  layer_dense(units = 16, activation = "relu") %>%
  layer_dropout(0.5) %>% 
  layer_dense(units = emoji_count, activation = "softmax")

model <- keras_model(input, output)
```

#### model 2 (0.33)
- add an extra layer

- accuracy on the dev test set is 33%
- loss is 2.6548331 
```{r eval = FALSE}
input <- layer_input(shape = c(1), dtype = "string")

output <- input %>% 
  text_vectorization() %>% 
  layer_embedding(input_dim = num_words + 1, output_dim = 16) %>%
  layer_global_average_pooling_1d() %>%
  layer_dense(units = 16, activation = "relu") %>%
  layer_dense(units = 16, activation = "relu") %>%
  layer_dropout(0.2) %>% 
  layer_dense(units = emoji_count, activation = "softmax")

model <- keras_model(input, output)
```

#### model 3 (0.34)
- add an extra layer
- 2 dropout layers
- double the size of the layers: 16 is small to go from 50 tot 50
- the more complex network seems only slightly more performant

- accuracy on the dev test set is 0.34
- loss is 2.5792720
```{r eval = FALSE}
input <- layer_input(shape = c(1), dtype = "string")

output <- input %>% 
  text_vectorization() %>% 
  layer_embedding(input_dim = num_words + 1, output_dim = 16) %>%
  layer_global_average_pooling_1d() %>%
  layer_dense(units = 32, activation = "relu") %>%
  layer_dropout(0.1) %>% 
  layer_dense(units = 32, activation = "relu") %>%
  layer_dropout(0.1) %>% 
  layer_dense(units = 32, activation = "relu") %>%
  layer_dense(units = emoji_count, activation = "softmax")

model <- keras_model(input, output)
```

#### model 4 (0.36)
-the input and output are both size 50 so let's make the middle layers also size 50
- 2 dense layers
- 1 small dropout 

- accuracy on the dev test set is 0.3578
- accuracy on the full set is 0.36157 with loss 2.4654262 
- loss is 2.5344746
```{r eval = FALSE}
input <- layer_input(shape = c(1), dtype = "string")

output <- input %>% 
  text_vectorization() %>% 
  layer_embedding(input_dim = num_words + 1, output_dim = 16) %>%
  layer_global_average_pooling_1d() %>%
  layer_dense(units = 50, activation = "relu") %>%
  layer_dense(units = 50, activation = "relu") %>%
  layer_dropout(0.2) %>% 
  layer_dense(units = emoji_count, activation = "softmax")

model <- keras_model(input, output)
```

#### model 5 (0.34)
- 50 is perhaps a little big
- lets try a deeper more slender model

- accuracy on the dev test set is 0.3398407 
- loss is 2.6135445 
```{r eval = FALSE}
input <- layer_input(shape = c(1), dtype = "string")

output <- input %>% 
  text_vectorization() %>% 
  layer_embedding(input_dim = num_words + 1, output_dim = 16) %>%
  layer_global_average_pooling_1d() %>%
  layer_dense(units = 32, activation = "relu") %>%
  layer_dense(units = 32, activation = "relu") %>%
  layer_dropout(0.1) %>% 
  layer_dense(units = 32, activation = "relu") %>%
  layer_dropout(0.1) %>% 
  layer_dense(units = 32, activation = "relu") %>%
  layer_dropout(0.1) %>% 
  layer_dense(units = emoji_count, activation = "softmax")

model <- keras_model(input, output)
```



#### model 6 (0.36)
- three layers of 40 each 

- took 3 hours to train on the full train set
- accuracy on the full set is 0.3630368 
- with loss 2.4692233 
```{r eval = TRUE}
input <- layer_input(shape = c(1), dtype = "string")

output <- input %>% 
  text_vectorization() %>% 
  layer_embedding(input_dim = num_words + 1, output_dim = 16) %>%
  layer_global_average_pooling_1d() %>%
  layer_dense(units = 40, activation = "relu") %>%
  layer_dense(units = 40, activation = "relu") %>%
  layer_dense(units = 40, activation = "relu") %>%
  layer_dropout(0.2) %>% 
  layer_dense(units = emoji_count, activation = "softmax")

model <- keras_model(input, output)
```

#### model 7 (0.377)

- modify the layer embedding from 16 to 40

- text vectorization with max length 100
  - perf on dev set loss: 2.6350586 accuracy: 0.3726049
  - run time 47 minutes
  
- text vectorization with max length 50
  - perf on dev set loss: 2.6026843  accuracy: 0.3773668 
  - run time 44 minutes
```{r eval = FALSE}
input <- layer_input(shape = c(1), dtype = "string")

output <- input %>% 
  text_vectorization() %>% 
  layer_embedding(input_dim = num_words + 1, output_dim = 40) %>%
  layer_global_average_pooling_1d() %>%
  layer_dense(units = 40, activation = "relu") %>%
  layer_dense(units = 40, activation = "relu") %>%
  layer_dense(units = 40, activation = "relu") %>%
  layer_dropout(0.2) %>% 
  layer_dense(units = emoji_count, activation = "softmax")

model <- keras_model(input, output)
```

#### model 8 (0.362)

- same amount of nodes but distributed over more layers is slower and worse

- perf on dev set loss: 2.6350629    accuracy: 0.3620144
- run time 1.05 hours
```{r eval = FALSE}
input <- layer_input(shape = c(1), dtype = "string")

output <- input %>% 
  text_vectorization() %>% 
  layer_embedding(input_dim = num_words + 1, output_dim = 40) %>%
  layer_global_average_pooling_1d() %>%
  layer_dense(units = 30, activation = "relu") %>%
  layer_dense(units = 30, activation = "relu") %>%
  layer_dense(units = 30, activation = "relu") %>%
  layer_dense(units = 30, activation = "relu") %>%
  layer_dropout(0.2) %>% 
  layer_dense(units = emoji_count, activation = "softmax")

model <- keras_model(input, output)
```

#### model 9 (0.387)

- 4 x 50 nodes

- perf on dev set loss: 2.6434860 accuracy:  0.3869762
- the val_loss was rising, this is a sign of over fitting

```{r eval=FALSE}
input <- layer_input(shape = c(1), dtype = "string")

output <- input %>% 
  text_vectorization() %>% 
  layer_embedding(input_dim = num_words + 1, output_dim = 50) %>%
  layer_global_average_pooling_1d() %>%
  layer_dense(units = 50, activation = "relu") %>%
  layer_dense(units = 50, activation = "relu") %>%
  layer_dense(units = 50, activation = "relu") %>%
  layer_dropout(0.2) %>% 
  layer_dense(units = emoji_count, activation = "softmax")

model <- keras_model(input, output)
```

#### model 10 (0.373)

- 4 x 50 nodes
- double dropout (0.2-> 0.4) because last seemed to over fit (val_loss was increasing)
- change epochs from 100-50 because it didn't do much it the later epochs
- this is a little less performing than 2.3 but trains twice as fast and show less signs of over fitting

- perf on dev set loss: 2.6168747 accuracy:  0.3732527
- run time 26 minutes
```{r eval = FALSE}
input <- layer_input(shape = c(1), dtype = "string")

output <- input %>% 
  text_vectorization() %>% 
  layer_embedding(input_dim = num_words + 1, output_dim = 50) %>%
  layer_global_average_pooling_1d() %>%
  layer_dense(units = 50, activation = "relu") %>%
  layer_dense(units = 50, activation = "relu") %>%
  layer_dense(units = 50, activation = "relu") %>%
  layer_dropout(0.4) %>% 
  layer_dense(units = emoji_count, activation = "softmax")

model <- keras_model(input, output)
```

#### model 11 (0.374)

- 5 x 50 nodes
- add an extra layer 
- nothing changes
- this model is larger so seems to need more epochs: train for another 50 epochs
- more epoch had no significant impact on performance, the loss is even higher

50 epochs 
- perf on dev set loss: 2.6722088  accuracy: 0.3744027
- run time 24 minutes 

100 epochs 
- perf on dev set loss: 2.859356  accuracy: 0.379285 
- run time another 24 minutes 
```{r}
input <- layer_input(shape = c(1), dtype = "string")

output <- input %>% 
  text_vectorization() %>% 
  layer_embedding(input_dim = num_words + 1, output_dim = 50) %>%
  layer_global_average_pooling_1d() %>%
  layer_dense(units = 50, activation = "relu") %>%
  layer_dense(units = 50, activation = "relu") %>%
  layer_dense(units = 50, activation = "relu") %>%
  layer_dropout(0.4) %>% 
  layer_dense(units = emoji_count, activation = "softmax")

model <- keras_model(input, output)
```



### compile model with loss function and optimizer
the tutorial uses binary_crossentropy but our variables are categorical
```{r}
model %>% compile(
  optimizer = 'adam',
  loss = 'categorical_crossentropy',
  metrics = list('accuracy')
)
```

### Train the model

```{r}
t1<-Sys.time()
```

```{r}
history <- model %>% fit(
  training$text,
  oh_train,
  epochs = 100,
  batch_size = 512,
  validation_split = 0.2,
  verbose=2
)
```

```{r}
time <- Sys.time()-t1
time
```

### evaluate model
```{r}
results <- model %>% evaluate(testing$text, oh_test, verbose = 0)
results
```

```{r eval=FALSE}
plot(history)
```

### single prediction

- basis for an application
```{r eval = FALSE}

id <-10
prediction <- predict(model,testing$text[id])[1,]
predicted_int <- which.max(prediction)
predicted_emoji <- emoji_dict[predicted_int]

print(paste("predicted:",predicted_emoji))

print(paste("true:",testing$emoji[id]))


```


```{r}
plot(prediction)
```



## lets try some different things

> THIS LINE IS TO FIND THIS PLACE IN THE NOTEBOOK

- convolutional networks
- find similar projects on github
- [LSTM example in python](https://medium.com/@djajafer/multi-class-text-classification-with-keras-and-lstm-4c5525bef592)
- training on the full data set takes 3 hours so I will use the dev for development
- go back to creating cleaner dataset, lemmatization, remove nonsense, check if that gives improvement

### input vectorization
- double the size of input vector
```{r}
num_words <- 10000
max_length <- 50
text_vectorization <- layer_text_vectorization(
  max_tokens = num_words, 
  output_sequence_length = max_length, 
)
text_vectorization %>% adapt(tweetDT$text)
```

#### model x

```{r}

```


### compile model
```{r}
model %>% compile(
  optimizer = 'adam',
  loss = 'categorical_crossentropy',
  metrics = list('accuracy')
)
```

### Train the model

```{r}
t1<-Sys.time()
t1
```

```{r}
history <- model %>% fit(
  training$text,
  oh_train,
  epochs = 50,
  batch_size = 512,
  validation_split = 0.2,
  verbose=2
)
```

```{r}
time <- Sys.time()-t1
time
```

### evaluate model
```{r}
results <- model %>% evaluate(testing$text, oh_test, verbose = 0)
results
```






