---
title: "Tweet analysis"
author: "Tijs Van den Heuvel"
date: "10/15/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(reticulate)
library(magrittr)
library(data.table)
library(NLP)
library(tm)
```

# Tweet sentiment analysis and emoji prediction

The purpose of this project is to try and predict an emoji based on text.

It would be great if an algorithm could suggest some emojis based on a different piece of text.

The data set consists of a collection of tweets with at least one emoji.

[data source](https://www.kaggle.com/rexhaif/emojifydata-en)

The data is gathered, selected and reformatted by _Daniil Larionov_ from the _ArchiveTeam Twitter data_ source

I transformed this data to turn it into a data table in this jupyter notebook: 
[preprocess](tweet-data-prepros.ipynb)
with the use of PySpark and regex
- this transformation process took a lot of work to make fast enough for large data sets
- so I did it for 10 and 165 tweets to get started 
- I did some initial experimentation and data cleaning with the 10 tweets set
- then I tried to create and run a model with the 165 tweets set
- when that model worked (with an accuracy of 18%) I proceeded to fix the pre-processor & get the entire data set


```{r}
#tweetDT <- fread("./data/reduced_tweets_single.csv")
#tweetDT <- fread("./data/165tweets.csv")
tweetDT <- fread("./data/tweets_full_v2.csv")
nrow(tweetDT)
```

## a sample of the tweets

```{r}
tweetDT[sample(1:.N,3)]
```

## clean text

- remove all non ascii characters
- remove numbers 
- remove words of length 1 
- merge multiple spaces
- remove trailing spaces
- to lower case

> some of these steps could remove too much information, can be revised

> remove trailing space

> do this is fewer steps because this is slow

```{r}
tweetDT$text<-gsub('[^\x01-\x7F]',' ',tweetDT$text)
tweetDT$text<-gsub('[0-9]+',' ',tweetDT$text)
tweetDT$text<-gsub('\\b\\w{1}\\s','',tweetDT$text)
tweetDT$text<-gsub('\\s+',' ',tweetDT$text)
tweetDT$text<-gsub(' $','',tweetDT$text)
tweetDT$text<-tolower(tweetDT$text)

```

```{r}
tweetDT[sample(1:.N,10)]
```

## convert to Corpus to us tm package

- text cleaning can be done with regex in data table
- I could use the tm package if I would want to analyse the data in a different way

```{r eval=FALSE}
corp = VCorpus(VectorSource(tweetDT$text))
writeLines(as.character(corp[[1]]))

corp <- tm_map(corp, removePunctuation)
corp <- tm_map(corp,content_transformer(tolower))

removeChar <- content_transformer(function(x, pattern) {return (gsub(pattern," ", x))})

corp <- tm_map(corp, removeChar,"%")
```


## distribution of number of words in a tweet 

```{r}
tweetDT$text %>% strsplit(" ") %>% sapply(length) %>% summary()
```

## how many different emoji's are present
```{r}
emoji_count <- unique(tweetDT$emoji)%>%length()
emoji_count
```

## Text classification with Tensorflow for R

- here I start with some tensorflow tutorial
  - [tutorial](https://tensorflow.rstudio.com/tutorials/beginners/basic-ml/tutorial_basic_text_classification/)
- the input is the text of the tweet
- the output should be the emoji
- a neural network takes in tensors (matrix) and not text so I needed to prepare the data

```{r}
library(keras)
library(dplyr)
library(ggplot2)
library(purrr)
```

### prepare the input data

- the tweets must be converted to tensors to use in NN
- create a dictionary & represent each of the 10,000 most common words by an integer
- you could create a very sparse 1,0000D vector but that's too memory intensive (one hot)
- alternative solution from tutorial:
  - pad the arrays so they all have same length
  - create integer tensor of shape `num_examples * max_length`
  - use an embedding layer capable of handling this shape as first layer
- `adapt` the text vectorization layer to learn about unique words in our data set

```{r}
num_words <- 10000
max_length <- 50
text_vectorization <- layer_text_vectorization(
  max_tokens = num_words, 
  output_sequence_length = max_length, 
)
text_vectorization %>% adapt(tweetDT$text)
```

show vocabulary 
```{r eval=FALSE}
get_vocabulary(text_vectorization)%>%sample(10)
```

show how text vectorization layer transforms input
```{r eval=FALSE}
text_vectorization(matrix(tweetDT$text[1], ncol = 1))
```

### prepare the output data

- I searched a long time how to encode the emojis
- the emojis have to be encoded as a vector
- one hot is a good method 
- keras had built in function: to_categorical but it needs integer input
- convert labels to integers
- first column is entirely empty so I remove it
- how many emojis are there -> size of final layer
- create an emoji dictionary to find which number maps to which emoji after prediction

```{r}
emoji_int <- match(tweetDT$emoji,unique(tweetDT$emoji))

one_hot_emoji <- to_categorical(emoji_int)
one_hot_emoji <- one_hot_emoji[,2:(emoji_count+1)]

emoji_dict <- tweetDT$emoji%>%unique()
```

### split the dataset into training & testing

- 80% training, 20% testing, arbitrarily chosen

```{r}
training_id <- sample.int(nrow(tweetDT), size = nrow(tweetDT)*0.8)
training <- tweetDT[training_id,]
testing <- tweetDT[-training_id,]
oh_train <-one_hot_emoji[training_id,]
oh_test <- one_hot_emoji[-training_id,]
nrow(training)
nrow(oh_train)
nrow(testing)
nrow(oh_test)
```

### build the model

- how many layers?
- how many hidden units?
- what types of building blocks?

model 1
- accuracy on test set of 29%
- loss of 2.783
```{r eval = FALSE}
input <- layer_input(shape = c(1), dtype = "string")

output <- input %>% 
  text_vectorization() %>% 
  layer_embedding(input_dim = num_words + 1, output_dim = 16) %>%
  layer_global_average_pooling_1d() %>%
  layer_dense(units = 16, activation = "relu") %>%
  layer_dropout(0.5) %>% 
  layer_dense(units = emoji_count, activation = "softmax")

model <- keras_model(input, output)
```

model 2
- add an extra layer
- accuracy on test set is 33%
- loss is 2.6548331 
```{r eval = FALSE}
input <- layer_input(shape = c(1), dtype = "string")

output <- input %>% 
  text_vectorization() %>% 
  layer_embedding(input_dim = num_words + 1, output_dim = 16) %>%
  layer_global_average_pooling_1d() %>%
  layer_dense(units = 16, activation = "relu") %>%
  layer_dense(units = 16, activation = "relu") %>%
  layer_dropout(0.2) %>% 
  layer_dense(units = emoji_count, activation = "softmax")

model <- keras_model(input, output)
```

model 3
- add an extra layer
- 2 dropout layers
- double the size of the layers: 16 is small to go from 50 tot 50
- accuracy on test set is 0.34
- loss is 2.5792720
- the more complex network is only slightly more performant
```{r eval = FALSE}
input <- layer_input(shape = c(1), dtype = "string")

output <- input %>% 
  text_vectorization() %>% 
  layer_embedding(input_dim = num_words + 1, output_dim = 16) %>%
  layer_global_average_pooling_1d() %>%
  layer_dense(units = 32, activation = "relu") %>%
  layer_dropout(0.1) %>% 
  layer_dense(units = 32, activation = "relu") %>%
  layer_dropout(0.1) %>% 
  layer_dense(units = 32, activation = "relu") %>%
  layer_dense(units = emoji_count, activation = "softmax")

model <- keras_model(input, output)
```

model 4
-the input and output are both size 50 so let's make the middle layers also size 50
- 2 dense layers
- 1 small dropout 
- accuracy on test set is 0.3578
- loss is 2.5344746
```{r eval = FALSE}
input <- layer_input(shape = c(1), dtype = "string")

output <- input %>% 
  text_vectorization() %>% 
  layer_embedding(input_dim = num_words + 1, output_dim = 16) %>%
  layer_global_average_pooling_1d() %>%
  layer_dense(units = 50, activation = "relu") %>%
  layer_dense(units = 50, activation = "relu") %>%
  layer_dropout(0.2) %>% 
  layer_dense(units = emoji_count, activation = "softmax")

model <- keras_model(input, output)
```

model 5
- 50 is perhaps a little big
- lets try a deeper more slender model
- accuracy on test set is 0.3398407 
- loss is 2.6135445 
```{r eval = TRUE}
input <- layer_input(shape = c(1), dtype = "string")

output <- input %>% 
  text_vectorization() %>% 
  layer_embedding(input_dim = num_words + 1, output_dim = 16) %>%
  layer_global_average_pooling_1d() %>%
  layer_dense(units = 32, activation = "relu") %>%
  layer_dense(units = 32, activation = "relu") %>%
  layer_dropout(0.1) %>% 
  layer_dense(units = 32, activation = "relu") %>%
  layer_dropout(0.1) %>% 
  layer_dense(units = 32, activation = "relu") %>%
  layer_dropout(0.1) %>% 
  layer_dense(units = emoji_count, activation = "softmax")

model <- keras_model(input, output)
```

### loss function and optimizer

- the tutorial uses binary_crossentropy but our variables are categorical

```{r}
model %>% compile(
  optimizer = 'adam',
  loss = 'categorical_crossentropy',
  metrics = list('accuracy')
)
```

### Train the model

```{r}
history <- model %>% fit(
  training$text,
  oh_train,
  epochs = 100,
  batch_size = 512,
  validation_split = 0.2,
  verbose=2
)
```

### evaluate model
```{r}
results <- model %>% evaluate(testing$text, oh_test, verbose = 0)
results
```

```{r}
plot(history)
```

```{r}
testing[sample(1:.N,10)]
```

```{r}

id <-10
prediction <- predict(model,testing$text[id])[1,]
predicted_int <- which.max(prediction)
predicted_emoji <- emoji_dict[predicted_int]

plot(prediction)

print(paste("predicted:",predicted_emoji))

print(paste("true:",testing$emoji[id]))


```


```{r}
plot(prediction)
```















